vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6, cuda=True, d_embedding=300, d_hidden=500, d_out=1837, dev_every=500, device=1, dropout_prob=0.5487647674511446, epochs=25, gpu=0, log_every=250, lr=2.3368165563254845e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.519370
    28     0       250  4.217004
    70     0       500  3.454021  0.464165    0.460844
    97     1       750  2.695011
   139     1      1000  2.023616  0.582841    0.579520
   167     2      1250  1.947864
   209     2      1500  2.173526  0.654498    0.643415
   236     2      1750  2.050010
   278     3      2000  1.698692  0.691334    0.676525
   305     3      2250  1.334085
   347     4      2500  1.258004  0.721398    0.703776
   375     4      2750  1.318608
   417     5      3000  1.237201  0.745152    0.725632
   445     5      3250  1.053011
   488     5      3500  1.393011  0.762713    0.740141
   516     6      3750  1.064027
   558     6      4000  1.188779  0.780143    0.753441
   586     7      4250  1.180950
   629     7      4500  1.158713  0.792488    0.765811
   657     8      4750  1.092308
   699     8      5000  0.772544  0.804582    0.771205
   728     8      5250  0.935450
   770     9      5500  1.031198  0.814555    0.776786
   798     9      5750  0.795584
   840    10      6000  0.759640  0.824318    0.785156
   868    10      6250  0.861564
   910    10      6500  0.605637  0.833289    0.787388
   938    11      6750  0.720791
   980    11      7000  0.615107  0.841089    0.791295
  1007    12      7250  0.765555
  1050    12      7500  0.601661  0.847874    0.800037
  1078    13      7750  0.523285
  1120    13      8000  0.550137  0.855265    0.800130
  1148    13      8250  0.722784
  1190    14      8500  0.632320  0.860231    0.802269
  1218    14      8750  0.631018
  1261    15      9000  0.584924  0.867174    0.804781
  1289    15      9250  0.341058
  1331    16      9500  0.483922  0.872853    0.809431
  1359    16      9750  0.433858
  1402    16     10000  0.514899  0.878004    0.809152
  1430    17     10250  0.388845
  1472    17     10500  0.379268  0.884301    0.809338
  1500    18     10750  0.504304
  1542    18     11000  0.477527  0.887898    0.807385
  1570    18     11250  0.496538
  1612    19     11500  0.457043  0.894630    0.808780
  1639    19     11750  0.351258
  1682    20     12000  0.567525  0.900348    0.809896
  1710    20     12250  0.422562
  1752    21     12500  0.327270  0.904247    0.810919
  1779    21     12750  0.443917
  1821    21     13000  0.429144  0.907726    0.811663
  1849    22     13250  0.339602
  1891    22     13500  0.393765  0.913101    0.812035
  1919    23     13750  0.493038
  1961    23     14000  0.322024  0.916473    0.810361
  1989    24     14250  0.394269
  2031    24     14500  0.315911  0.920623    0.814081
  2059    24     14750  0.302249
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6, cuda=True, d_embedding=300, d_hidden=500, d_out=1837, dev_every=500, device=1, dropout_prob=0.5235083503167401, epochs=25, gpu=0, log_every=250, lr=9.543880052712213e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.562489
    28     0       250  2.717191
    70     0       500  1.675420  0.683917    0.673270
    98     1       750  1.317849
   140     1      1000  1.132524  0.757101    0.731678
   168     2      1250  1.179604
   210     2      1500  0.727408  0.802843    0.771484
   238     2      1750  1.093024
   280     3      2000  0.711856  0.829522    0.785156
   308     3      2250  0.868195
   351     4      2500  0.789406  0.848229    0.793062
   379     4      2750  0.748050
   421     5      3000  0.530056  0.864856    0.803478
   449     5      3250  0.489331
   492     5      3500  0.791100  0.880520    0.802362
   520     6      3750  0.684948
   562     6      4000  0.574934  0.893853    0.801804
   590     7      4250  0.449654
   632     7      4500  0.407685  0.905407    0.804781
   660     8      4750  0.298767
   702     8      5000  0.382648  0.917765    0.803850
   730     8      5250  0.311796
   772     9      5500  0.240214  0.928199    0.802083
   799     9      5750  0.403507
   842    10      6000  0.195193  0.942216    0.804222
   869    10      6250  0.360780
   911    10      6500  0.179228  0.949542    0.801060
   939    11      6750  0.244037
   981    11      7000  0.239074  0.957828    0.798642
  1009    12      7250  0.187415
  1051    12      7500  0.242319  0.964113    0.796038
  1079    13      7750  0.205365
  1121    13      8000  0.188374  0.970542    0.795294
  1149    13      8250  0.218278
  1191    14      8500  0.165913  0.973585    0.797433
  1218    14      8750  0.181852
  1261    15      9000  0.171079  0.977406    0.795666
  1288    15      9250  0.153494
  1331    16      9500  0.063608  0.982017    0.789528
  1358    16      9750  0.129606
  1401    16     10000  0.067068  0.983940    0.789249
  1428    17     10250  0.056862
  1470    17     10500  0.113310  0.985640    0.790365
  1498    18     10750  0.082778
  1540    18     11000  0.108898  0.986483    0.792225
  1568    18     11250  0.156388
  1610    19     11500  0.074055  0.988406    0.784598
  1638    19     11750  0.110450
  1680    20     12000  0.103423  0.988709    0.787481
  1707    20     12250  0.072456
  1749    21     12500  0.057874  0.991015    0.786086
  1777    21     12750  0.036475
  1819    21     13000  0.082274  0.991792    0.788876
  1847    22     13250  0.047618
  1889    22     13500  0.054384  0.992108    0.785714
  1917    23     13750  0.035868
  1959    23     14000  0.062694  0.991502    0.790365
  1987    24     14250  0.084560
  2029    24     14500  0.061790  0.994809    0.785714
  2056    24     14750  0.044786
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6, cuda=True, d_embedding=300, d_hidden=500, d_out=1837, dev_every=500, device=1, dropout_prob=0.5973369506283627, epochs=25, gpu=0, log_every=250, lr=6.789914913570565e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.538278
    28     0       250  2.793929
    70     0       500  2.281076  0.605212    0.600074
    98     1       750  1.900491
   140     1      1000  1.823353  0.709607    0.691778
   167     2      1250  1.095232
   209     2      1500  1.101957  0.756521    0.735026
   237     2      1750  1.370564
   279     3      2000  0.972925  0.788891    0.762184
   307     3      2250  0.815842
   349     4      2500  1.124413  0.810511    0.773996
   376     4      2750  0.545115
   419     5      3000  0.806257  0.829363    0.786086
   446     5      3250  0.610705
   489     5      3500  0.797700  0.845858    0.794829
   516     6      3750  0.497445
   558     6      4000  0.694232  0.855396    0.797805
   586     7      4250  0.741005
   629     7      4500  0.607206  0.867873    0.801897
   657     8      4750  0.565121
   699     8      5000  0.472905  0.879690    0.804315
   727     8      5250  0.708608
   769     9      5500  0.614066  0.889242    0.803757
   797     9      5750  0.456040
   839    10      6000  0.432193  0.899241    0.806920
   866    10      6250  0.382535
   908    10      6500  0.394834  0.909333    0.807292
   935    11      6750  0.367661
   978    11      7000  0.637153  0.918950    0.808594
  1005    12      7250  0.394814
  1047    12      7500  0.319768  0.927514    0.810919
  1075    13      7750  0.262185
  1117    13      8000  0.243790  0.932639    0.804688
  1144    13      8250  0.240168
  1187    14      8500  0.224746  0.939858    0.806734
  1215    14      8750  0.280511
  1257    15      9000  0.223731  0.948698    0.801339
  1284    15      9250  0.342026
  1327    16      9500  0.219288  0.954956    0.801525
  1354    16      9750  0.144273
  1396    16     10000  0.268068  0.960437    0.799014
  1424    17     10250  0.142029
  1466    17     10500  0.204999  0.964903    0.800688
  1494    18     10750  0.188032
  1536    18     11000  0.197725  0.968842    0.801153
  1563    18     11250  0.163313
  1606    19     11500  0.213809  0.970818    0.796875
  1633    19     11750  0.167466
  1675    20     12000  0.110523  0.974349    0.793992
  1702    20     12250  0.145699
  1744    21     12500  0.074068  0.977392    0.794085
  1772    21     12750  0.098240
  1814    21     13000  0.112673  0.983084    0.794550
  1842    22     13250  0.106451
  1884    22     13500  0.222595  0.982254    0.793992
  1912    23     13750  0.079745
  1954    23     14000  0.101426  0.985060    0.792318
  1982    24     14250  0.118253
  2024    24     14500  0.109737  0.986101    0.791853
  2052    24     14750  0.081499
