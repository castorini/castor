vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6802238665607186, cuda=True, d_embedding=300, d_hidden=573, d_out=1837, dev_every=500, dropout_prob=0.5864334720570055, epochs=50, gpu=0, log_every=250, lr=1.9722857731511165e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.615795
    35     0       250  4.400221
    89     0       500  3.102463  0.445049    0.444754
   124     1       750  2.927750
   179     1      1000  2.323624  0.560735    0.558222
   213     2      1250  2.041283
   268     2      1500  2.144501  0.628439    0.621373
   303     2      1750  1.548658
   357     3      2000  1.844281  0.676354    0.663132
   391     3      2250  1.313965
   446     4      2500  1.926213  0.703283    0.687872
   481     4      2750  1.119081
   535     5      3000  1.333975  0.728130    0.710193
   570     5      3250  1.163673
   624     5      3500  1.229059  0.745191    0.726376
   659     6      3750  1.126209
   713     6      4000  1.113246  0.762872    0.740234
   747     7      4250  1.100750
   802     7      4500  1.184020  0.776336    0.749442
   837     8      4750  1.129640
   891     8      5000  1.306347  0.789813    0.760789
   926     8      5250  1.040277
   980     9      5500  1.393736  0.799286    0.769531
  1015     9      5750  0.994933
  1069    10      6000  0.933759  0.808376    0.774275
  1104    10      6250  0.992596
  1158    10      6500  0.942163  0.815807    0.780041
  1193    11      6750  0.788935
  1247    11      7000  0.700171  0.825003    0.785807
  1282    12      7250  0.882477
  1336    12      7500  0.800062  0.831853    0.790737
  1371    13      7750  0.768172
  1425    13      8000  0.818355  0.840614    0.793620
  1460    13      8250  0.580523
  1514    14      8500  0.838755  0.845963    0.797061
  1548    14      8750  0.449995
  1603    15      9000  0.645428  0.852696    0.800874
  1637    15      9250  0.581343
  1691    16      9500  0.761380  0.856832    0.799572
  1725    16      9750  0.463649
  1780    16     10000  0.419242  0.862800    0.803106
  1814    17     10250  0.491795
  1869    17     10500  0.460914  0.868136    0.804874
  1903    18     10750  0.687135
  1957    18     11000  0.561422  0.872747    0.807664
  1992    18     11250  0.527030
  2046    19     11500  0.405738  0.877543    0.808687
  2081    19     11750  0.519011
  2135    20     12000  0.524446  0.882760    0.811849
  2170    20     12250  0.426626
  2224    21     12500  0.393198  0.888530    0.809245
  2258    21     12750  0.654639
  2312    21     13000  0.365668  0.893576    0.812221
  2347    22     13250  0.458114
  2401    22     13500  0.568364  0.897845    0.810919
  2435    23     13750  0.648356
  2490    23     14000  0.480464  0.902008    0.810640
  2524    24     14250  0.639504
  2578    24     14500  0.417330  0.904669    0.813802
  2613    24     14750  0.402847
  2667    25     15000  0.431719  0.907791    0.812035
  2702    25     15250  0.479038
  2756    26     15500  0.372902  0.911533    0.814267
  2791    26     15750  0.441641
  2845    26     16000  0.562970  0.916197    0.812500
  2879    27     16250  0.408939
  2933    27     16500  0.547633  0.920162    0.811849
  2967    28     16750  0.320347
  3022    28     17000  0.377969  0.922402    0.812965
  3056    29     17250  0.429984
  3111    29     17500  0.268052  0.928238    0.810733
  3145    29     17750  0.324523
  3199    30     18000  0.276304  0.931347    0.811477
  3233    30     18250  0.298586
  3289    31     18500  0.333077  0.933798    0.813523
  3323    31     18750  0.374802
  3377    32     19000  0.246738  0.938725    0.809896
  3411    32     19250  0.302809
  3466    32     19500  0.285517  0.940504    0.810547
  3500    33     19750  0.276121
  3554    33     20000  0.300188  0.945444    0.813802
  3588    34     20250  0.296287
  3643    34     20500  0.300198  0.945958    0.811942
  3677    34     20750  0.251707
  3731    35     21000  0.149845  0.949212    0.812407
  3765    35     21250  0.249836
  3820    36     21500  0.233051  0.952374    0.807571
  3854    36     21750  0.214599
  3909    37     22000  0.213690  0.953705    0.810082
  3943    37     22250  0.191133
  3998    37     22500  0.169076  0.958144    0.812035
  4032    38     22750  0.213568
  4087    38     23000  0.250642  0.960529    0.811291
  4121    39     23250  0.165977
  4175    39     23500  0.224193  0.961873    0.808129
  4209    40     23750  0.237195
  4264    40     24000  0.227691  0.963507    0.807013
  4298    40     24250  0.308375
  4353    41     24500  0.196988  0.965628    0.808036
  4387    41     24750  0.226173
  4442    42     25000  0.202577  0.966392    0.808129
  4476    42     25250  0.130012
  4530    43     25500  0.135420  0.970897    0.807292
  4565    43     25750  0.168683
  4619    43     26000  0.181689  0.971082    0.807106
  4653    44     26250  0.171279
  4708    44     26500  0.159324  0.974099    0.805711
  4742    45     26750  0.113605
  4796    45     27000  0.161601  0.976523    0.804129
  4831    45     27250  0.169415
  4885    46     27500  0.125197  0.977432    0.806641
  4919    46     27750  0.125446
  4974    47     28000  0.133849  0.977432    0.805246
  5008    47     28250  0.080398
  5063    48     28500  0.136535  0.979935    0.804781
  5096    48     28750  0.076769
  5151    48     29000  0.156327  0.981411    0.801525
  5186    49     29250  0.110160
  5240    49     29500  0.103809  0.981740    0.805990
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.626142588175356, cuda=True, d_embedding=300, d_hidden=593, d_out=1837, dev_every=500, dropout_prob=0.5334866388404141, epochs=50, gpu=0, log_every=250, lr=9.38834698927289e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.579207
    48     0       250  2.177398
   125     0       500  1.456428  0.712215    0.692522
   174     1       750  1.446215
   252     1      1000  1.198608  0.783134    0.754464
   300     2      1250  0.752240
   378     2      1500  0.898314  0.816531    0.775949
   426     2      1750  0.773782
   504     3      2000  0.528749  0.844870    0.795573
   552     3      2250  0.712297
   629     4      2500  0.654189  0.858426    0.794457
   678     4      2750  0.680279
   755     5      3000  0.597114  0.878004    0.801618
   804     5      3250  0.428103
   881     5      3500  0.389769  0.891376    0.799293
   929     6      3750  0.604201
  1006     6      4000  0.450717  0.905354    0.805246
  1055     7      4250  0.260185
  1132     7      4500  0.584343  0.919148    0.801246
  1180     8      4750  0.323299
  1257     8      5000  0.339962  0.925814    0.802176
  1306     8      5250  0.280672
  1382     9      5500  0.281202  0.942559    0.801339
  1431     9      5750  0.268960
  1508    10      6000  0.370649  0.950595    0.801432
  1557    10      6250  0.217029
  1633    10      6500  0.196844  0.959739    0.796038
  1681    11      6750  0.154873
  1758    11      7000  0.224207  0.962163    0.795852
  1807    12      7250  0.149815
  1884    12      7500  0.129094  0.967064    0.793992
  1932    13      7750  0.135211
  2010    13      8000  0.143352  0.973137    0.794271
  2058    13      8250  0.155270
  2135    14      8500  0.254470  0.977696    0.792411
  2183    14      8750  0.121401
  2261    15      9000  0.056274  0.981582    0.789807
  2309    15      9250  0.090853
  2386    16      9500  0.098371  0.984678    0.787202
  2434    16      9750  0.103003
  2511    16     10000  0.095117  0.984678    0.789156
  2559    17     10250  0.130804
  2636    17     10500  0.070995  0.987497    0.788504
  2685    18     10750  0.084523
  2762    18     11000  0.169894  0.989882    0.784412
  2810    18     11250  0.056957
  2887    19     11500  0.040644  0.989698    0.788318
  2935    19     11750  0.064587
  3013    20     12000  0.066427  0.992135    0.790551
  3061    20     12250  0.062891
  3138    21     12500  0.033927  0.990580    0.792132
  3186    21     12750  0.061931
  3263    21     13000  0.100950  0.992530    0.789435
  3312    22     13250  0.053145
  3389    22     13500  0.069556  0.994243    0.785714
  3437    23     13750  0.028568
  3515    23     14000  0.022265  0.992780    0.780041
  3563    24     14250  0.011724
  3640    24     14500  0.108021  0.993913    0.789528
  3688    24     14750  0.025065
  3765    25     15000  0.035919  0.993610    0.783110
  3813    25     15250  0.007675
  3890    26     15500  0.028093  0.995073    0.784784
  3938    26     15750  0.028514
  4015    26     16000  0.017129  0.994388    0.785714
  4063    27     16250  0.022719
  4140    27     16500  0.020447  0.996179    0.783203
  4188    28     16750  0.034563
  4265    28     17000  0.033247  0.995969    0.784691
  4313    29     17250  0.015399
  4390    29     17500  0.010836  0.995534    0.778553
  4438    29     17750  0.005999
  4516    30     18000  0.010719  0.995481    0.782831
  4563    30     18250  0.045759
  4640    31     18500  0.006154  0.996733    0.785342
  4688    31     18750  0.007587
  4764    32     19000  0.015360  0.996957    0.786272
  4812    32     19250  0.017115
  4889    32     19500  0.016865  0.996680    0.779855
  4937    33     19750  0.051457
  5014    33     20000  0.009358  0.996100    0.774554
  5062    34     20250  0.006936
  5139    34     20500  0.013540  0.996614    0.783668
  5187    34     20750  0.017713
  5263    35     21000  0.007773  0.996864    0.783761
  5312    35     21250  0.006123
  5388    36     21500  0.033804  0.996601    0.782459
  5436    36     21750  0.022969
  5513    37     22000  0.011604  0.997167    0.779948
  5561    37     22250  0.033676
  5636    37     22500  0.023025  0.997418    0.784226
  5684    38     22750  0.006839
  5759    38     23000  0.037751  0.997444    0.779483
  5807    39     23250  0.002717
  5882    39     23500  0.009862  0.996983    0.778646
  5931    40     23750  0.062761
  6005    40     24000  0.001290  0.997326    0.782459
  6053    40     24250  0.008712
  6128    41     24500  0.001472  0.997523    0.781715
  6175    41     24750  0.012419
  6250    42     25000  0.017488  0.997787    0.782180
  6298    42     25250  0.002001
  6373    43     25500  0.046333  0.997194    0.782552
  6421    43     25750  0.007789
  6496    43     26000  0.043821  0.997734    0.784040
  6544    44     26250  0.001435
  6619    44     26500  0.003908  0.997615    0.780785
  6667    45     26750  0.001824
  6742    45     27000  0.025800  0.998261    0.784784
  6790    45     27250  0.006839
  6865    46     27500  0.117700  0.997971    0.779576
  6913    46     27750  0.036611
  6988    47     28000  0.002297  0.997747    0.783668
  7035    47     28250  0.002231
  7109    48     28500  0.004257  0.997892    0.782273
  7157    48     28750  0.007807
  7231    48     29000  0.002139  0.997839    0.778739
  7279    49     29250  0.003142
  7353    49     29500  0.010584  0.997826    0.779669
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6789481513994571, cuda=True, d_embedding=300, d_hidden=551, d_out=1837, dev_every=500, dropout_prob=0.5171070694045453, epochs=50, gpu=0, log_every=250, lr=2.5487429380114487e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.555103
    44     0       250  3.793747
   112     0       500  2.565926  0.545044    0.541109
   156     1       750  1.961618
   225     1      1000  2.096020  0.658134    0.646856
   268     2      1250  1.569557
   337     2      1500  1.824712  0.711056    0.694754
   381     2      1750  1.423977
   450     3      2000  1.209059  0.745573    0.724516
   493     3      2250  1.390933
   561     4      2500  1.215288  0.770368    0.744699
   605     4      2750  1.032362
   673     5      3000  0.961618  0.792409    0.760603
   717     5      3250  0.980174
   786     5      3500  1.320558  0.808258    0.773903
   830     6      3750  0.901660
   898     6      4000  0.795126  0.822052    0.783575
   942     7      4250  0.719658
  1010     7      4500  0.695341  0.832894    0.785993
  1054     8      4750  0.712879
  1122     8      5000  0.551703  0.843368    0.794085
  1166     8      5250  0.584767
  1235     9      5500  0.657533  0.854448    0.797619
  1279     9      5750  0.595273
  1347    10      6000  0.585834  0.860640    0.801060
  1391    10      6250  0.454468
  1460    10      6500  0.599238  0.870033    0.806827
  1504    11      6750  0.575430
  1573    11      7000  0.584057  0.876120    0.806362
  1617    12      7250  0.577450
  1685    12      7500  0.540734  0.884499    0.808594
  1729    13      7750  0.515660
  1797    13      8000  0.406197  0.891112    0.808129
  1841    13      8250  0.512276
  1909    14      8500  0.526354  0.895539    0.810547
  1953    14      8750  0.315847
  2022    15      9000  0.419531  0.901929    0.813988
  2066    15      9250  0.436963
  2134    16      9500  0.302074  0.910980    0.813058
  2178    16      9750  0.317602
  2246    16     10000  0.313517  0.916170    0.812221
  2290    17     10250  0.433081
  2358    17     10500  0.399552  0.920716    0.811384
  2402    18     10750  0.230393
  2470    18     11000  0.340793  0.925722    0.812128
  2514    18     11250  0.230808
  2582    19     11500  0.292363  0.931874    0.814825
  2627    19     11750  0.383791
  2695    20     12000  0.255955  0.937421    0.814546
  2739    20     12250  0.249689
  2807    21     12500  0.130229  0.940188    0.812593
  2851    21     12750  0.328624
  2919    21     13000  0.222612  0.945813    0.813895
  2962    22     13250  0.189258
  3031    22     13500  0.268818  0.949568    0.811384
  3075    23     13750  0.284651
  3143    23     14000  0.205353  0.953612    0.813337
  3187    24     14250  0.214927
  3255    24     14500  0.364926  0.955575    0.811570
  3299    24     14750  0.182201
  3367    25     15000  0.232368  0.959936    0.811105
  3411    25     15250  0.198287
  3479    26     15500  0.155187  0.962650    0.808315
  3523    26     15750  0.207109
  3591    26     16000  0.231946  0.966774    0.809431
  3635    27     16250  0.142226
  3704    27     16500  0.179535  0.970463    0.808408
  3747    28     16750  0.174995
  3816    28     17000  0.117513  0.970766    0.808036
  3859    29     17250  0.154732
  3927    29     17500  0.230933  0.975021    0.806734
  3971    29     17750  0.212116
  4040    30     18000  0.173940  0.976259    0.809152
  4083    30     18250  0.196719
  4152    31     18500  0.169993  0.976457    0.808966
  4195    31     18750  0.143759
  4263    32     19000  0.111887  0.982017    0.807478
  4307    32     19250  0.096008
  4375    32     19500  0.080252  0.983058    0.808501
  4418    33     19750  0.128777
  4487    33     20000  0.121210  0.982610    0.805432
  4530    34     20250  0.080750
  4598    34     20500  0.084527  0.984085    0.803385
  4642    34     20750  0.191220
  4710    35     21000  0.095554  0.986839    0.803292
  4754    35     21250  0.099542
  4822    36     21500  0.084440  0.988064    0.805153
  4866    36     21750  0.096186
  4933    37     22000  0.091520  0.988182    0.803106
  4977    37     22250  0.071374
  5046    37     22500  0.131379  0.988696    0.805618
  5089    38     22750  0.056623
  5158    38     23000  0.049117  0.989750    0.800874
  5201    39     23250  0.072337
  5269    39     23500  0.068406  0.991819    0.803571
  5313    40     23750  0.072312
  5381    40     24000  0.051876  0.991950    0.803292
  5425    40     24250  0.103129
  5493    41     24500  0.122524  0.992649    0.798363
  5537    41     24750  0.066024
  5605    42     25000  0.064335  0.993215    0.802455
  5649    42     25250  0.053490
  5717    43     25500  0.057559  0.992859    0.801246
  5761    43     25750  0.031644
  5829    43     26000  0.041776  0.994757    0.804222
  5873    44     26250  0.076066
  5941    44     26500  0.074418  0.994519    0.801804
  5984    45     26750  0.043048
  6052    45     27000  0.034159  0.995204    0.803199
  6096    45     27250  0.065078
  6164    46     27500  0.051064  0.994954    0.801153
  6208    46     27750  0.036337
  6276    47     28000  0.060720  0.996114    0.797619
  6320    47     28250  0.034709
  6388    48     28500  0.025957  0.995639    0.798642
  6432    48     28750  0.059084
  6500    48     29000  0.079348  0.995731    0.800502
  6544    49     29250  0.037023
  6612    49     29500  0.069180  0.996627    0.799479
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6518893884629545, cuda=True, d_embedding=300, d_hidden=588, d_out=1837, dev_every=500, dropout_prob=0.5204593157128516, epochs=50, gpu=0, log_every=250, lr=5.9920773112060856e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.622644
    36     0       250  2.875520
    90     0       500  1.692078  0.649571    0.638858
   126     1       750  1.702927
   180     1      1000  1.526757  0.741595    0.723028
   215     2      1250  1.351819
   270     2      1500  1.091934  0.781724    0.753999
   305     2      1750  1.188671
   360     3      2000  0.753149  0.808139    0.770554
   395     3      2250  1.003014
   450     4      2500  0.854380  0.833026    0.789900
   485     4      2750  0.693510
   540     5      3000  0.657494  0.846806    0.796503
   576     5      3250  0.775940
   630     5      3500  0.598426  0.862142    0.801432
   665     6      3750  0.592903
   720     6      4000  0.537534  0.876858    0.807943
   756     7      4250  0.433722
   810     7      4500  0.407071  0.889123    0.810082
   845     8      4750  0.402331
   900     8      5000  0.480721  0.897463    0.807385
   935     8      5250  0.506844
   990     9      5500  0.206130  0.907620    0.810733
  1025     9      5750  0.431319
  1080    10      6000  0.323473  0.916368    0.807199
  1115    10      6250  0.408239
  1170    10      6500  0.362021  0.925485    0.808966
  1205    11      6750  0.259461
  1259    11      7000  0.270860  0.934127    0.803664
  1294    12      7250  0.312926
  1349    12      7500  0.284249  0.943811    0.807385
  1384    13      7750  0.190175
  1438    13      8000  0.236256  0.950411    0.804781
  1473    13      8250  0.381824
  1528    14      8500  0.165904  0.957077    0.803850
  1563    14      8750  0.242593
  1617    15      9000  0.109313  0.962360    0.798549
  1652    15      9250  0.198672
  1707    16      9500  0.188410  0.966655    0.801153
  1742    16      9750  0.142392
  1797    16     10000  0.217587  0.970845    0.804315
  1832    17     10250  0.148241
  1886    17     10500  0.149894  0.975113    0.801246
  1921    18     10750  0.135835
  1976    18     11000  0.148742  0.977524    0.797898
  2011    18     11250  0.132913
  2066    19     11500  0.073141  0.980185    0.797247
  2101    19     11750  0.136277
  2156    20     12000  0.092953  0.982992    0.800502
  2191    20     12250  0.133410
  2245    21     12500  0.074491  0.983940    0.800316
  2280    21     12750  0.125974
  2335    21     13000  0.175958  0.986496    0.797154
  2370    22     13250  0.089308
  2424    22     13500  0.075175  0.987089    0.795573
  2459    23     13750  0.082341
  2514    23     14000  0.128020  0.989210    0.794922
  2549    24     14250  0.073832
  2604    24     14500  0.066828  0.990356    0.793713
  2639    24     14750  0.091563
  2694    25     15000  0.118988  0.989842    0.791481
  2729    25     15250  0.053149
  2783    26     15500  0.032038  0.992003    0.795294
  2818    26     15750  0.073672
  2873    26     16000  0.070875  0.994203    0.792225
  2908    27     16250  0.046033
  2962    27     16500  0.115629  0.993742    0.788597
  2998    28     16750  0.066064
  3052    28     17000  0.043087  0.993900    0.793248
  3087    29     17250  0.048101
  3142    29     17500  0.034869  0.994704    0.790737
  3177    29     17750  0.028884
  3231    30     18000  0.012840  0.995178    0.793806
  3266    30     18250  0.022102
  3321    31     18500  0.043680  0.995086    0.792225
  3356    31     18750  0.079667
  3410    32     19000  0.017708  0.995771    0.792504
  3446    32     19250  0.007442
  3500    32     19500  0.033734  0.995481    0.790365
  3535    33     19750  0.030844
  3589    33     20000  0.039919  0.995995    0.787388
  3625    34     20250  0.036584
  3679    34     20500  0.035920  0.995613    0.790086
  3714    34     20750  0.032500
  3768    35     21000  0.035100  0.996390    0.787109
  3804    35     21250  0.023731
  3858    36     21500  0.033381  0.996548    0.788039
  3893    36     21750  0.062199
  3948    37     22000  0.010356  0.996944    0.788597
  3982    37     22250  0.018335
  4037    37     22500  0.046187  0.996693    0.787946
  4072    38     22750  0.012306
  4126    38     23000  0.017112  0.997115    0.792039
  4161    39     23250  0.036104
  4215    39     23500  0.117970  0.997167    0.785621
  4250    40     23750  0.006860
  4305    40     24000  0.052403  0.997207    0.785528
  4339    40     24250  0.009809
  4394    41     24500  0.029166  0.997444    0.784877
  4429    41     24750  0.032041
  4483    42     25000  0.003419  0.997128    0.785714
  4518    42     25250  0.040321
  4572    43     25500  0.016379  0.997049    0.789807
  4607    43     25750  0.019983
  4662    43     26000  0.006213  0.997774    0.785156
  4697    44     26250  0.021335
  4751    44     26500  0.032625  0.997207    0.784319
  4786    45     26750  0.041077
  4840    45     27000  0.030527  0.997563    0.788318
  4875    45     27250  0.007864
  4929    46     27500  0.005115  0.997128    0.787946
  4964    46     27750  0.004534
  5019    47     28000  0.018478  0.997787    0.790737
  5053    47     28250  0.005684
  5108    48     28500  0.003145  0.997774    0.791295
  5142    48     28750  0.004017
  5197    48     29000  0.006874  0.997734    0.788225
  5231    49     29250  0.012303
  5286    49     29500  0.064703  0.997958    0.791853
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6631584745736081, cuda=True, d_embedding=300, d_hidden=587, d_out=1837, dev_every=500, dropout_prob=0.5014404716632638, epochs=50, gpu=0, log_every=250, lr=1.5322941082431096e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.570446
    48     0       250  3.906822
   122     0       500  2.926768  0.481701    0.479818
   169     1       750  2.287997
   244     1      1000  2.560144  0.597834    0.591983
   291     2      1250  2.055229
   365     2      1500  2.036209  0.664497    0.650670
   413     2      1750  1.804094
   487     3      2000  1.855234  0.703836    0.687593
   535     3      2250  1.258847
   609     4      2500  1.421942  0.729500    0.710100
   657     4      2750  1.388739
   730     5      3000  1.070330  0.752714    0.728981
   778     5      3250  1.121466
   852     5      3500  0.899460  0.770803    0.744978
   900     6      3750  1.439948
   974     6      4000  0.894598  0.783384    0.756138
  1022     7      4250  0.830311
  1096     7      4500  0.802415  0.796559    0.765067
  1143     8      4750  0.768276
  1217     8      5000  0.930261  0.807678    0.771484
  1265     8      5250  0.816128
  1339     9      5500  0.916918  0.817875    0.779576
  1387     9      5750  1.075021
  1461    10      6000  0.848661  0.827203    0.783296
  1509    10      6250  0.760782
  1583    10      6500  0.775867  0.834370    0.788876
  1631    11      6750  0.636178
  1705    11      7000  0.573607  0.842907    0.792597
  1752    12      7250  0.690329
  1826    12      7500  0.732808  0.849995    0.793992
  1873    13      7750  0.816455
  1948    13      8000  0.718488  0.854553    0.799851
  1995    13      8250  0.624606
  2069    14      8500  0.895397  0.862524    0.802641
  2117    14      8750  0.619572
  2191    15      9000  0.471659  0.868887    0.806455
  2238    15      9250  0.530769
  2313    16      9500  0.719957  0.873775    0.804967
  2360    16      9750  0.465865
  2434    16     10000  0.542933  0.880757    0.808036
  2481    17     10250  0.637145
  2556    17     10500  0.547720  0.883234    0.806548
  2603    18     10750  0.374640
  2677    18     11000  0.428169  0.888781    0.809431
  2725    18     11250  0.475164
  2799    19     11500  0.475044  0.894064    0.809989
  2846    19     11750  0.292028
  2920    20     12000  0.370928  0.899426    0.811663
  2968    20     12250  0.326392
  3042    21     12500  0.343325  0.903180    0.813523
  3090    21     12750  0.681752
  3164    21     13000  0.381978  0.907897    0.811942
  3211    22     13250  0.411723
  3285    22     13500  0.366621  0.912073    0.810082
  3332    23     13750  0.398952
  3406    23     14000  0.477962  0.916197    0.811291
  3454    24     14250  0.374060
  3528    24     14500  0.519928  0.920913    0.811942
  3575    24     14750  0.396394
  3649    25     15000  0.298571  0.924036    0.813523
  3696    25     15250  0.406497
  3770    26     15500  0.203060  0.929516    0.813895
  3817    26     15750  0.256189
  3891    26     16000  0.443031  0.931295    0.812593
  3939    27     16250  0.363714
  4013    27     16500  0.333199  0.936314    0.811012
  4060    28     16750  0.273946
  4134    28     17000  0.290976  0.939423    0.811942
  4181    29     17250  0.250076
  4255    29     17500  0.324206  0.942151    0.812872
  4302    29     17750  0.268312
  4376    30     18000  0.230559  0.944996    0.813616
  4424    30     18250  0.258383
  4498    31     18500  0.263182  0.948540    0.812407
  4545    31     18750  0.380785
  4619    32     19000  0.324400  0.950108    0.811477
  4666    32     19250  0.275084
  4740    32     19500  0.235761  0.954548    0.811663
  4788    33     19750  0.301147
  4862    33     20000  0.245435  0.956432    0.810082
  4909    34     20250  0.142666
  4983    34     20500  0.307822  0.958171    0.812035
  5030    34     20750  0.164618
  5104    35     21000  0.193503  0.960964    0.812965
  5151    35     21250  0.199683
  5225    36     21500  0.173774  0.963586    0.811942
  5272    36     21750  0.152608
  5346    37     22000  0.244652  0.965325    0.809338
  5394    37     22250  0.144964
  5468    37     22500  0.207756  0.968052    0.810640
  5515    38     22750  0.168775
  5589    38     23000  0.153955  0.969119    0.808222
  5636    39     23250  0.332366
  5710    39     23500  0.136290  0.971069    0.806920
  5758    40     23750  0.147497
  5832    40     24000  0.108332  0.973506    0.806362
  5879    40     24250  0.189476
  5953    41     24500  0.121796  0.975614    0.804874
  6001    41     24750  0.189983
  6074    42     25000  0.149896  0.977630    0.809338
  6121    42     25250  0.193008
  6196    43     25500  0.089163  0.978908    0.806176
  6243    43     25750  0.140560
  6317    43     26000  0.168882  0.980225    0.804688
  6364    44     26250  0.148757
  6438    44     26500  0.194362  0.981806    0.805618
  6485    45     26750  0.106816
  6559    45     27000  0.143587  0.982570    0.804408
  6607    45     27250  0.154296
  6681    46     27500  0.154980  0.984230    0.804874
  6728    46     27750  0.144185
  6802    47     28000  0.098829  0.985495    0.805432
  6849    47     28250  0.124042
  6923    48     28500  0.098317  0.985864    0.803571
  6971    48     28750  0.091306
  7045    48     29000  0.085605  0.987221    0.804688
  7092    49     29250  0.079033
  7166    49     29500  0.049927  0.987563    0.803385
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6601197405663987, cuda=True, d_embedding=300, d_hidden=561, d_out=1837, dev_every=500, dropout_prob=0.5696318823635655, epochs=50, gpu=0, log_every=250, lr=1.2904217945176189e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.625420
    45     0       250  4.691126
   114     0       500  3.399728  0.401652    0.403181
   159     1       750  3.189633
   229     1      1000  2.596193  0.525875    0.524833
   274     2      1250  2.467173
   344     2      1500  1.849158  0.600812    0.596819
   388     2      1750  1.610818
   458     3      2000  2.050785  0.646290    0.640253
   503     3      2250  1.831550
   573     4      2500  1.793989  0.679740    0.664528
   618     4      2750  1.648737
   688     5      3000  1.758281  0.705417    0.687407
   733     5      3250  1.565663
   802     5      3500  1.635543  0.724441    0.705450
   847     6      3750  1.714886
   916     6      4000  1.263222  0.740514    0.718471
   961     7      4250  1.311616
  1031     7      4500  1.325815  0.752411    0.730097
  1075     8      4750  1.032178
  1145     8      5000  0.988156  0.764861    0.740606
  1190     8      5250  1.373443
  1260     9      5500  1.019593  0.777126    0.751488
  1305     9      5750  1.034693
  1375    10      6000  1.285600  0.785084    0.759673
  1420    10      6250  1.227927
  1489    10      6500  0.894611  0.795044    0.765904
  1533    11      6750  1.210975
  1604    11      7000  1.052754  0.804121    0.771298
  1648    12      7250  1.001632
  1718    12      7500  0.935511  0.811907    0.776507
  1763    13      7750  0.913213
  1833    13      8000  1.115537  0.818784    0.780227
  1878    13      8250  0.911525
  1947    14      8500  0.907824  0.823606    0.784970
  1992    14      8750  0.868366
  2062    15      9000  0.769851  0.831326    0.788597
  2107    15      9250  0.757431
  2177    16      9500  0.954038  0.835898    0.789900
  2221    16      9750  0.892329
  2292    16     10000  0.980492  0.842274    0.794178
  2336    17     10250  0.862175
  2405    17     10500  0.912311  0.845990    0.794922
  2450    18     10750  0.716538
  2520    18     11000  0.541995  0.852616    0.799107
  2565    18     11250  0.767582
  2634    19     11500  0.643055  0.855633    0.801153
  2679    19     11750  0.849152
  2749    20     12000  0.633702  0.861246    0.800595
  2793    20     12250  0.604842
  2863    21     12500  0.596366  0.864421    0.804129
  2908    21     12750  0.552029
  2978    21     13000  0.495391  0.869124    0.805990
  3022    22     13250  0.518719
  3092    22     13500  0.654690  0.872589    0.807292
  3137    23     13750  0.394015
  3207    23     14000  0.541755  0.877556    0.810454
  3251    24     14250  0.399361
  3321    24     14500  0.477525  0.880560    0.809617
  3366    24     14750  0.467195
  3436    25     15000  0.672764  0.884025    0.811198
  3481    25     15250  0.383609
  3550    26     15500  0.481926  0.887529    0.812314
  3595    26     15750  0.366283
  3665    26     16000  0.416505  0.890520    0.811570
  3709    27     16250  0.444032
  3779    27     16500  0.570056  0.894973    0.814453
  3824    28     16750  0.542892
  3894    28     17000  0.438128  0.898345    0.813430
  3938    29     17250  0.341088
  4008    29     17500  0.420649  0.901296    0.812407
  4052    29     17750  0.319926
  4122    30     18000  0.413311  0.904814    0.814825
  4167    30     18250  0.350133
  4236    31     18500  0.397004  0.906724    0.815476
  4281    31     18750  0.383678
  4351    32     19000  0.303025  0.910927    0.816406
  4396    32     19250  0.373874
  4465    32     19500  0.403207  0.914497    0.815104
  4510    33     19750  0.397625
  4580    33     20000  0.368952  0.916487    0.814267
  4624    34     20250  0.383302
  4694    34     20500  0.315476  0.919530    0.815383
  4739    34     20750  0.462516
  4808    35     21000  0.413460  0.922665    0.813988
  4853    35     21250  0.391557
  4923    36     21500  0.257812  0.925669    0.815011
  4967    36     21750  0.159059
  5037    37     22000  0.284217  0.928212    0.815662
  5081    37     22250  0.316267
  5151    37     22500  0.338513  0.931835    0.815569
  5196    38     22750  0.388989
  5266    38     23000  0.317696  0.934654    0.814453
  5311    39     23250  0.229483
  5380    39     23500  0.257121  0.936235    0.815104
  5425    40     23750  0.215473
  5494    40     24000  0.277390  0.937329    0.812686
  5539    40     24250  0.333207
  5609    41     24500  0.209517  0.940267    0.816313
  5654    41     24750  0.350313
  5724    42     25000  0.218521  0.942072    0.814918
  5768    42     25250  0.244349
  5837    43     25500  0.256038  0.946669    0.814639
  5882    43     25750  0.286706
  5952    43     26000  0.212637  0.946432    0.814546
  5997    44     26250  0.293816
  6066    44     26500  0.251308  0.949001    0.814825
  6110    45     26750  0.328157
  6180    45     27000  0.259734  0.950200    0.814267
  6224    45     27250  0.326781
  6294    46     27500  0.300797  0.953033    0.815290
  6338    46     27750  0.159329
  6408    47     28000  0.292238  0.955062    0.811942
  6452    47     28250  0.245396
  6521    48     28500  0.268287  0.957657    0.812965
  6566    48     28750  0.266069
  6636    48     29000  0.218505  0.959370    0.813058
  6680    49     29250  0.155690
  6751    49     29500  0.235187  0.960990    0.812314
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6700674590954401, cuda=True, d_embedding=300, d_hidden=553, d_out=1837, dev_every=500, dropout_prob=0.5046777452700411, epochs=50, gpu=0, log_every=250, lr=1.3928443002583176e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.567289
    33     0       250  4.553061
    84     0       500  3.636390  0.423206    0.419922
   117     1       750  2.780717
   167     1      1000  2.581361  0.533977    0.531808
   200     2      1250  2.111333
   251     2      1500  2.460257  0.609401    0.605934
   284     2      1750  1.793805
   334     3      2000  1.774372  0.652416    0.645740
   367     3      2250  1.587064
   418     4      2500  1.504889  0.687763    0.675316
   451     4      2750  1.497947
   502     5      3000  1.452644  0.710938    0.696987
   535     5      3250  1.871244
   586     5      3500  1.283024  0.728143    0.710751
   618     6      3750  1.417506
   669     6      4000  1.125522  0.744256    0.722470
   702     7      4250  1.430161
   753     7      4500  1.431544  0.759855    0.734654
   786     8      4750  1.135475
   837     8      5000  1.333816  0.772647    0.747675
   869     8      5250  1.155295
   920     9      5500  0.950047  0.783305    0.754092
   953     9      5750  0.955981
  1004    10      6000  0.978493  0.791289    0.759673
  1037    10      6250  0.978244
  1088    10      6500  1.261459  0.802659    0.766927
  1121    11      6750  1.088593
  1172    11      7000  0.793233  0.808785    0.773251
  1205    12      7250  1.066997
  1256    12      7500  0.798599  0.818152    0.778367
  1288    13      7750  0.749315
  1339    13      8000  0.872113  0.824265    0.783668
  1372    13      8250  0.987660
  1423    14      8500  0.665344  0.830193    0.788039
  1455    14      8750  0.715711
  1506    15      9000  0.685632  0.836623    0.788876
  1539    15      9250  0.809288
  1590    16      9500  0.722069  0.842854    0.793341
  1623    16      9750  0.829375
  1674    16     10000  0.743656  0.847742    0.795480
  1707    17     10250  0.682811
  1758    17     10500  0.823147  0.853539    0.798642
  1790    18     10750  0.735239
  1841    18     11000  0.645436  0.858835    0.799014
  1874    18     11250  0.545722
  1925    19     11500  0.901367  0.862616    0.802920
  1958    19     11750  0.475745
  2008    20     12000  0.643404  0.866845    0.801711
  2041    20     12250  0.514945
  2092    21     12500  0.529285  0.871114    0.803385
  2125    21     12750  0.569111
  2176    21     13000  0.530453  0.876370    0.807478
  2209    22     13250  0.607037
  2259    22     13500  0.511200  0.880428    0.807385
  2292    23     13750  0.519910
  2342    23     14000  0.619544  0.884947    0.809896
  2375    24     14250  0.388618
  2426    24     14500  0.396247  0.888319    0.808873
  2458    24     14750  0.463817
  2509    25     15000  0.379873  0.892469    0.810361
  2542    25     15250  0.510428
  2593    26     15500  0.368492  0.894024    0.810547
  2626    26     15750  0.449531
  2677    26     16000  0.430804  0.899320    0.812407
  2710    27     16250  0.548921
  2761    27     16500  0.420038  0.902667    0.813523
  2794    28     16750  0.344733
  2844    28     17000  0.441969  0.906896    0.812035
  2877    29     17250  0.326295
  2928    29     17500  0.427673  0.908845    0.812872
  2961    29     17750  0.425863
  3011    30     18000  0.429116  0.912284    0.813895
  3044    30     18250  0.389006
  3095    31     18500  0.304123  0.915446    0.813523
  3128    31     18750  0.315697
  3179    32     19000  0.297738  0.920189    0.815476
  3212    32     19250  0.596751
  3263    32     19500  0.402572  0.922086    0.813802
  3295    33     19750  0.375304
  3346    33     20000  0.203578  0.925735    0.813151
  3379    34     20250  0.486053
  3429    34     20500  0.329582  0.928554    0.813058
  3462    34     20750  0.343465
  3513    35     21000  0.343427  0.930834    0.813895
  3546    35     21250  0.255881
  3597    36     21500  0.324543  0.934378    0.813523
  3629    36     21750  0.349004
  3680    37     22000  0.251984  0.937592    0.815383
  3713    37     22250  0.297361
  3764    37     22500  0.241418  0.939595    0.814639
  3796    38     22750  0.224820
  3847    38     23000  0.219981  0.941795    0.814174
  3880    39     23250  0.313268
  3931    39     23500  0.385862  0.944351    0.815197
  3963    40     23750  0.220422
  4014    40     24000  0.252061  0.947315    0.813151
  4047    40     24250  0.279725
  4098    41     24500  0.247673  0.950912    0.811570
  4130    41     24750  0.214085
  4181    42     25000  0.176049  0.952400    0.814267
  4213    42     25250  0.249691
  4264    43     25500  0.291431  0.955536    0.811849
  4297    43     25750  0.143821
  4348    43     26000  0.250761  0.956181    0.813895
  4381    44     26250  0.159509
  4431    44     26500  0.332228  0.958948    0.811570
  4464    45     26750  0.167279
  4515    45     27000  0.280623  0.960476    0.812314
  4547    45     27250  0.250167
  4598    46     27500  0.176418  0.960938    0.813988
  4630    46     27750  0.177563
  4681    47     28000  0.171962  0.963190    0.813616
  4714    47     28250  0.217660
  4765    48     28500  0.121744  0.964916    0.811291
  4798    48     28750  0.198535
  4848    48     29000  0.160168  0.966682    0.814453
  4881    49     29250  0.183165
  4932    49     29500  0.100214  0.969527    0.810826
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6785539789197366, cuda=True, d_embedding=300, d_hidden=550, d_out=1837, dev_every=500, dropout_prob=0.5978199592050236, epochs=50, gpu=0, log_every=250, lr=3.831291237075416e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.534818
    33     0       250  3.270363
    84     0       500  2.843409  0.536111    0.538039
   116     1       750  2.090446
   167     1      1000  1.969790  0.657185    0.649182
   200     2      1250  1.549501
   250     2      1500  1.467583  0.710740    0.696708
   283     2      1750  1.797386
   333     3      2000  1.223428  0.743281    0.724702
   366     3      2250  1.251060
   417     4      2500  1.167437  0.771316    0.748047
   450     4      2750  1.240957
   500     5      3000  1.046933  0.790946    0.762835
   533     5      3250  0.904366
   583     5      3500  0.920937  0.809417    0.777251
   616     6      3750  0.633039
   666     6      4000  1.066475  0.820220    0.780692
   699     7      4250  0.783310
   749     7      4500  0.897594  0.834001    0.790737
   782     8      4750  0.711758
   833     8      5000  0.780700  0.845107    0.795759
   866     8      5250  0.836989
   916     9      5500  0.696179  0.853670    0.799107
   949     9      5750  0.768059
  1000    10      6000  0.699038  0.864052    0.803664
  1033    10      6250  0.704789
  1083    10      6500  0.538506  0.871496    0.804967
  1116    11      6750  0.636619
  1166    11      7000  0.479286  0.878544    0.805525
  1199    12      7250  0.492096
  1249    12      7500  0.558802  0.885079    0.809059
  1281    13      7750  0.419178
  1332    13      8000  0.368686  0.891468    0.809989
  1365    13      8250  0.426423
  1415    14      8500  0.514356  0.899267    0.809617
  1447    14      8750  0.425507
  1498    15      9000  0.383378  0.907146    0.811291
  1530    15      9250  0.431277
  1581    16      9500  0.369217  0.912363    0.812872
  1613    16      9750  0.376821
  1664    16     10000  0.253059  0.918054    0.811477
  1696    17     10250  0.303911
  1746    17     10500  0.290111  0.923430    0.811477
  1779    18     10750  0.276417
  1829    18     11000  0.264039  0.928699    0.811384
  1861    18     11250  0.446574
  1911    19     11500  0.334675  0.934444    0.812128
  1944    19     11750  0.298009
  1994    20     12000  0.201859  0.940583    0.804781
  2027    20     12250  0.256924
  2078    21     12500  0.322550  0.946142    0.807664
  2110    21     12750  0.221736
  2160    21     13000  0.306244  0.951505    0.808501
  2192    22     13250  0.386369
  2243    22     13500  0.212422  0.955365    0.809617
  2275    23     13750  0.297653
  2326    23     14000  0.347911  0.957631    0.810268
  2358    24     14250  0.154881
  2408    24     14500  0.137652  0.961135    0.806269
  2441    24     14750  0.169243
  2491    25     15000  0.278099  0.964402    0.808780
  2524    25     15250  0.271486
  2574    26     15500  0.174444  0.968447    0.805246
  2606    26     15750  0.177751
  2656    26     16000  0.106168  0.972426    0.804315
  2689    27     16250  0.154796
  2739    27     16500  0.172557  0.973967    0.803571
  2772    28     16750  0.175895
  2822    28     17000  0.151318  0.976615    0.804781
  2854    29     17250  0.089456
  2905    29     17500  0.138982  0.978670    0.800688
  2937    29     17750  0.171127
  2987    30     18000  0.138530  0.980792    0.803106
  3020    30     18250  0.117059
  3070    31     18500  0.105226  0.982359    0.800781
  3102    31     18750  0.173623
  3153    32     19000  0.072166  0.982162    0.803385
  3185    32     19250  0.116272
  3236    32     19500  0.078117  0.985758    0.798921
  3268    33     19750  0.061983
  3318    33     20000  0.098269  0.986707    0.801432
  3351    34     20250  0.121298
  3401    34     20500  0.091678  0.987471    0.799107
  3434    34     20750  0.083484
  3484    35     21000  0.071811  0.986351    0.796038
  3517    35     21250  0.130274
  3567    36     21500  0.095372  0.990145    0.798270
  3599    36     21750  0.102478
  3649    37     22000  0.048687  0.991120    0.798456
  3682    37     22250  0.086433
  3732    37     22500  0.107074  0.991819    0.798177
  3765    38     22750  0.052685
  3815    38     23000  0.072594  0.992596    0.797712
  3848    39     23250  0.074968
  3898    39     23500  0.107452  0.992583    0.797061
  3931    40     23750  0.058439
  3981    40     24000  0.082307  0.993268    0.797247
  4013    40     24250  0.031684
  4064    41     24500  0.055069  0.993057    0.797061
  4096    41     24750  0.030217
  4146    42     25000  0.040722  0.993953    0.797061
  4179    42     25250  0.026261
  4229    43     25500  0.057345  0.994480    0.794457
  4262    43     25750  0.077715
  4312    43     26000  0.080060  0.994849    0.797805
  4344    44     26250  0.032223
  4395    44     26500  0.077818  0.994796    0.794736
  4427    45     26750  0.034815
  4477    45     27000  0.113088  0.995718    0.793341
  4510    45     27250  0.047407
  4560    46     27500  0.034435  0.994361    0.796131
  4592    46     27750  0.022765
  4643    47     28000  0.030411  0.996034    0.795573
  4675    47     28250  0.018267
  4726    48     28500  0.017056  0.995810    0.796782
  4758    48     28750  0.030642
  4808    48     29000  0.050081  0.995810    0.794364
  4841    49     29250  0.058391
  4891    49     29500  0.037541  0.995626    0.797712
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6673694701023971, cuda=True, d_embedding=300, d_hidden=553, d_out=1837, dev_every=500, dropout_prob=0.5533872517426502, epochs=50, gpu=0, log_every=250, lr=1.023961630649494e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.542372
    33     0       250  4.960388
    84     0       500  4.004858  0.331893    0.340309
   117     1       750  3.823303
   168     1      1000  3.223149  0.455207    0.454985
   201     2      1250  2.821049
   252     2      1500  3.258334  0.528773    0.528646
   285     2      1750  2.364242
   336     3      2000  2.639859  0.580141    0.577288
   369     3      2250  2.010652
   420     4      2500  2.265837  0.620863    0.615420
   453     4      2750  2.500253
   504     5      3000  1.871626  0.650335    0.642857
   537     5      3250  1.836378
   588     5      3500  1.515673  0.673930    0.664714
   621     6      3750  1.794497
   672     6      4000  1.812003  0.691044    0.678199
   705     7      4250  1.450654
   756     7      4500  1.530090  0.706116    0.690755
   789     8      4750  1.099596
   840     8      5000  1.842160  0.719923    0.702660
   873     8      5250  1.372417
   924     9      5500  1.344017  0.730554    0.710938
   957     9      5750  1.321291
  1008    10      6000  1.597030  0.742807    0.722098
  1041    10      6250  1.230540
  1092    10      6500  1.269421  0.751028    0.730376
  1125    11      6750  1.599827
  1176    11      7000  1.144347  0.762213    0.737072
  1209    12      7250  1.057080
  1260    12      7500  0.934436  0.769867    0.746001
  1293    13      7750  1.170059
  1344    13      8000  1.271186  0.777548    0.750465
  1377    13      8250  1.118720
  1429    14      8500  1.089162  0.783529    0.756138
  1461    14      8750  1.024053
  1512    15      9000  0.742799  0.789972    0.761347
  1545    15      9250  1.082382
  1596    16      9500  1.139262  0.797560    0.763300
  1629    16      9750  1.008800
  1681    16     10000  0.946957  0.803607    0.768601
  1714    17     10250  0.733140
  1765    17     10500  0.785409  0.809325    0.773717
  1798    18     10750  0.874096
  1849    18     11000  1.096584  0.814503    0.775577
  1882    18     11250  0.860031
  1933    19     11500  0.613307  0.818573    0.781343
  1966    19     11750  0.798376
  2017    20     12000  0.929289  0.823606    0.781529
  2050    20     12250  0.753627
  2101    21     12500  0.782451  0.828336    0.786272
  2134    21     12750  0.825344
  2185    21     13000  0.694251  0.832525    0.785714
  2218    22     13250  0.602590
  2269    22     13500  0.857874  0.836807    0.788039
  2302    23     13750  0.630467
  2353    23     14000  0.605785  0.841339    0.792504
  2386    24     14250  0.643154
  2438    24     14500  0.759571  0.844857    0.793155
  2470    24     14750  0.891384
  2522    25     15000  0.676225  0.847860    0.793899
  2554    25     15250  0.714170
  2606    26     15500  0.572905  0.852893    0.795387
  2639    26     15750  0.873682
  2690    26     16000  0.537538  0.855594    0.801804
  2723    27     16250  0.663087
  2774    27     16500  0.517567  0.857478    0.799758
  2807    28     16750  0.689883
  2858    28     17000  0.661937  0.862497    0.801339
  2891    29     17250  0.761314
  2942    29     17500  0.695428  0.864315    0.803106
  2975    29     17750  0.663823
  3026    30     18000  0.476497  0.868162    0.803757
  3059    30     18250  0.610550
  3110    31     18500  0.521512  0.870639    0.805153
  3143    31     18750  0.412520
  3194    32     19000  0.452255  0.873893    0.804967
  3227    32     19250  0.541321
  3278    32     19500  0.520877  0.877688    0.806083
  3310    33     19750  0.482737
  3362    33     20000  0.453511  0.879809    0.808036
  3395    34     20250  0.474057
  3446    34     20500  0.420741  0.881890    0.807850
  3478    34     20750  0.412013
  3529    35     21000  0.612063  0.885619    0.808501
  3562    35     21250  0.589046
  3613    36     21500  0.683287  0.888319    0.807199
  3646    36     21750  0.645563
  3697    37     22000  0.374973  0.890941    0.810826
  3730    37     22250  0.472523
  3781    37     22500  0.519352  0.893444    0.810454
  3814    38     22750  0.419103
  3865    38     23000  0.582288  0.895803    0.810547
  3898    39     23250  0.506998
  3949    39     23500  0.595929  0.898530    0.810826
  3981    40     23750  0.524813
  4032    40     24000  0.344346  0.901138    0.811663
  4065    40     24250  0.460978
  4116    41     24500  0.426810  0.902627    0.810361
  4149    41     24750  0.422335
  4200    42     25000  0.544320  0.904998    0.811477
  4233    42     25250  0.426155
  4283    43     25500  0.216554  0.907488    0.811663
  4316    43     25750  0.573793
  4367    43     26000  0.432357  0.909899    0.812593
  4400    44     26250  0.482482
  4451    44     26500  0.448530  0.912706    0.812128
  4484    45     26750  0.323022
  4535    45     27000  0.306159  0.914313    0.812965
  4568    45     27250  0.335192
  4619    46     27500  0.339309  0.916539    0.811570
  4651    46     27750  0.213505
  4702    47     28000  0.461484  0.919306    0.812686
  4735    47     28250  0.343811
  4786    48     28500  0.386051  0.920992    0.814825
  4819    48     28750  0.426949
  4870    48     29000  0.366382  0.923469    0.813244
  4902    49     29250  0.255528
  4953    49     29500  0.361068  0.924945    0.813709
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6441460540822492, cuda=True, d_embedding=300, d_hidden=578, d_out=1837, dev_every=500, dropout_prob=0.5313124521290948, epochs=50, gpu=0, log_every=250, lr=6.544433095393454e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.574531
    47     0       250  2.476158
   120     0       500  1.598850  0.684299    0.672619
   166     1       750  1.267946
   239     1      1000  1.251343  0.760922    0.739490
   285     2      1250  1.008085
   358     2      1500  0.850662  0.796480    0.765160
   406     2      1750  1.117067
   478     3      2000  1.066431  0.822276    0.785249
   525     3      2250  0.949615
   598     4      2500  0.588828  0.845555    0.792225
   645     4      2750  0.838736
   718     5      3000  0.612797  0.859401    0.797061
   765     5      3250  0.673983
   837     5      3500  0.680650  0.874750    0.802734
   884     6      3750  0.651654
   957     6      4000  0.597004  0.887186    0.805897
  1004     7      4250  0.566715
  1077     7      4500  0.549642  0.896356    0.800874
  1124     8      4750  0.414623
  1197     8      5000  0.472730  0.908700    0.800781
  1243     8      5250  0.372727
  1316     9      5500  0.475998  0.922020    0.802920
  1362     9      5750  0.298724
  1436    10      6000  0.261794  0.930992    0.804315
  1482    10      6250  0.260909
  1555    10      6500  0.294681  0.937065    0.798270
  1601    11      6750  0.272505
  1674    11      7000  0.235423  0.946129    0.800502
  1721    12      7250  0.324563
  1794    12      7500  0.219612  0.952875    0.800130
  1840    13      7750  0.152764
  1913    13      8000  0.141703  0.959014    0.801897
  1960    13      8250  0.174980
  2033    14      8500  0.234456  0.965496    0.800409
  2079    14      8750  0.179260
  2153    15      9000  0.191986  0.971148    0.802920
  2199    15      9250  0.251544
  2272    16      9500  0.162631  0.976312    0.791853
  2319    16      9750  0.144278
  2392    16     10000  0.108296  0.978091    0.798549
  2438    17     10250  0.110470
  2511    17     10500  0.054606  0.980541    0.797433
  2558    18     10750  0.073878
  2631    18     11000  0.101173  0.983585    0.786923
  2677    18     11250  0.138457
  2751    19     11500  0.120334  0.983967    0.795201
  2797    19     11750  0.107725
  2870    20     12000  0.117716  0.986417    0.791760
  2917    20     12250  0.041362
  2990    21     12500  0.060370  0.989460    0.791853
  3037    21     12750  0.059366
  3110    21     13000  0.048183  0.990672    0.789435
  3156    22     13250  0.064153
  3229    22     13500  0.081660  0.990211    0.788783
  3276    23     13750  0.039816
  3349    23     14000  0.109297  0.991884    0.789993
  3396    24     14250  0.041207
  3469    24     14500  0.083393  0.991450    0.793992
  3515    24     14750  0.031500
  3588    25     15000  0.045501  0.993149    0.787946
  3635    25     15250  0.066000
  3708    26     15500  0.090041  0.993057    0.788225
  3755    26     15750  0.057053
  3828    26     16000  0.033318  0.992029    0.783668
  3874    27     16250  0.031800
  3947    27     16500  0.030106  0.994309    0.790551
  3993    28     16750  0.029839
  4066    28     17000  0.027509  0.994098    0.785900
  4113    29     17250  0.034324
  4185    29     17500  0.033604  0.995284    0.791667
  4232    29     17750  0.028738
  4305    30     18000  0.123196  0.993307    0.786551
  4351    30     18250  0.039293
  4424    31     18500  0.071567  0.994809    0.789807
  4471    31     18750  0.015910
  4544    32     19000  0.037232  0.996482    0.789807
  4590    32     19250  0.030181
  4662    32     19500  0.015058  0.996443    0.784133
  4709    33     19750  0.012463
  4782    33     20000  0.092984  0.996034    0.787946
  4828    34     20250  0.026828
  4901    34     20500  0.010229  0.996588    0.784691
  4947    34     20750  0.011358
  5020    35     21000  0.006398  0.997312    0.786365
  5066    35     21250  0.028450
  5139    36     21500  0.008945  0.996390    0.786179
  5186    36     21750  0.014489
  5258    37     22000  0.013476  0.996469    0.784505
  5305    37     22250  0.010312
  5377    37     22500  0.015626  0.996061    0.787109
  5424    38     22750  0.009134
  5497    38     23000  0.029396  0.995731    0.778925
  5543    39     23250  0.007331
  5616    39     23500  0.052128  0.996258    0.779855
  5662    40     23750  0.015112
  5735    40     24000  0.006496  0.997945    0.783575
  5781    40     24250  0.010185
  5854    41     24500  0.014755  0.997510    0.784319
  5900    41     24750  0.003917
  5973    42     25000  0.006231  0.997141    0.781808
  6019    42     25250  0.016536
  6091    43     25500  0.015414  0.997813    0.782459
  6138    43     25750  0.076976
  6210    43     26000  0.004783  0.997787    0.782738
  6256    44     26250  0.026096
  6329    44     26500  0.062632  0.997365    0.786179
  6375    45     26750  0.003042
  6448    45     27000  0.002099  0.998037    0.785435
  6493    45     27250  0.041258
  6566    46     27500  0.002530  0.997484    0.781994
  6612    46     27750  0.001516
  6685    47     28000  0.001190  0.997431    0.780692
  6731    47     28250  0.008022
  6803    48     28500  0.025728  0.997984    0.784784
  6849    48     28750  0.005271
  6922    48     29000  0.012126  0.998287    0.788690
  6968    49     29250  0.001629
  7040    49     29500  0.002629  0.997853    0.781808
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6477554249285173, cuda=True, d_embedding=300, d_hidden=563, d_out=1837, dev_every=500, dropout_prob=0.5504925632952284, epochs=50, gpu=0, log_every=250, lr=6.492025215987308e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.567974
    34     0       250  2.657539
    86     0       500  1.683068  0.650124    0.640253
   119     1       750  1.697895
   171     1      1000  1.449976  0.732531    0.715495
   204     2      1250  1.215496
   256     2      1500  1.018446  0.775506    0.750093
   290     2      1750  1.172039
   342     3      2000  0.854549  0.809602    0.775670
   375     3      2250  0.832475
   426     4      2500  1.096096  0.828230    0.785249
   460     4      2750  0.664122
   512     5      3000  0.808275  0.844764    0.793248
   545     5      3250  0.720326
   597     5      3500  0.703392  0.859902    0.801990
   631     6      3750  0.733790
   682     6      4000  0.610816  0.875698    0.804501
   716     7      4250  0.650866
   768     7      4500  0.712465  0.884407    0.807199
   801     8      4750  0.373772
   853     8      5000  0.421484  0.895961    0.808780
   887     8      5250  0.441161
   939     9      5500  0.459721  0.904287    0.808129
   972     9      5750  0.456791
  1024    10      6000  0.430753  0.915196    0.809338
  1057    10      6250  0.273359
  1109    10      6500  0.412861  0.923535    0.807664
  1142    11      6750  0.305251
  1194    11      7000  0.382998  0.932204    0.808873
  1227    12      7250  0.204145
  1279    12      7500  0.341306  0.939819    0.805525
  1312    13      7750  0.263737
  1364    13      8000  0.276659  0.948935    0.804222
  1397    13      8250  0.200980
  1448    14      8500  0.299248  0.956616    0.801525
  1482    14      8750  0.219083
  1533    15      9000  0.233111  0.958289    0.804129
  1567    15      9250  0.201681
  1619    16      9500  0.181045  0.964099    0.803292
  1652    16      9750  0.205441
  1703    16     10000  0.203648  0.971451    0.804594
  1737    17     10250  0.200873
  1789    17     10500  0.163893  0.973572    0.796968
  1822    18     10750  0.119190
  1874    18     11000  0.167160  0.977353    0.789714
  1907    18     11250  0.101843
  1959    19     11500  0.072349  0.975443    0.791946
  1992    19     11750  0.119099
  2044    20     12000  0.189278  0.983519    0.796317
  2077    20     12250  0.083618
  2129    21     12500  0.037925  0.985429    0.796689
  2162    21     12750  0.041694
  2214    21     13000  0.086606  0.987655    0.792132
  2247    22     13250  0.093850
  2298    22     13500  0.136515  0.986733    0.792225
  2332    23     13750  0.119272
  2384    23     14000  0.150426  0.989579    0.796038
  2417    24     14250  0.091323
  2469    24     14500  0.051567  0.990541    0.792411
  2502    24     14750  0.085635
  2553    25     15000  0.049506  0.990765    0.792876
  2587    25     15250  0.091232
  2639    26     15500  0.063501  0.990883    0.793899
  2672    26     15750  0.055782
  2723    26     16000  0.057150  0.991766    0.793155
  2757    27     16250  0.075838
  2808    27     16500  0.038835  0.992649    0.785621
  2841    28     16750  0.037836
  2893    28     17000  0.059733  0.992938    0.792783
  2926    29     17250  0.037361
  2978    29     17500  0.069013  0.994071    0.790179
  3012    29     17750  0.046131
  3063    30     18000  0.050253  0.994124    0.792690
  3096    30     18250  0.076084
  3148    31     18500  0.074822  0.995152    0.786644
  3181    31     18750  0.059366
  3233    32     19000  0.045191  0.996324    0.788504
  3266    32     19250  0.014881
  3318    32     19500  0.033597  0.994533    0.784784
  3351    33     19750  0.112872
  3403    33     20000  0.054166  0.995639    0.785063
  3436    34     20250  0.045712
  3488    34     20500  0.075316  0.995718    0.787946
  3521    34     20750  0.029417
  3573    35     21000  0.057727  0.995033    0.784970
  3606    35     21250  0.039953
  3658    36     21500  0.021045  0.996272    0.788318
  3691    36     21750  0.024101
  3743    37     22000  0.011500  0.996944    0.791760
  3776    37     22250  0.005412
  3827    37     22500  0.049185  0.996970    0.790179
  3860    38     22750  0.016723
  3912    38     23000  0.014630  0.996430    0.787295
  3945    39     23250  0.016963
  3997    39     23500  0.035607  0.996983    0.789621
  4030    40     23750  0.008553
  4081    40     24000  0.025085  0.996680    0.787016
  4115    40     24250  0.006349
  4166    41     24500  0.085113  0.996680    0.785528
  4199    41     24750  0.042110
  4251    42     25000  0.011526  0.996640    0.787853
  4284    42     25250  0.008198
  4335    43     25500  0.015558  0.996640    0.787202
  4368    43     25750  0.031070
  4420    43     26000  0.029209  0.997365    0.787202
  4453    44     26250  0.003461
  4504    44     26500  0.004433  0.997958    0.785621
  4537    45     26750  0.039472
  4589    45     27000  0.003580  0.997853    0.787667
  4622    45     27250  0.002728
  4673    46     27500  0.009629  0.997497    0.786365
  4706    46     27750  0.011731
  4758    47     28000  0.002713  0.997602    0.784412
  4790    47     28250  0.005473
  4841    48     28500  0.006142  0.997839    0.783017
  4874    48     28750  0.003890
  4926    48     29000  0.007487  0.997932    0.784226
  4959    49     29250  0.006196
  5010    49     29500  0.015659  0.997418    0.782924
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6628707783931327, cuda=True, d_embedding=300, d_hidden=570, d_out=1837, dev_every=500, dropout_prob=0.5396827300082907, epochs=50, gpu=0, log_every=250, lr=3.7288309578296835e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.550604
    46     0       250  3.310847
   116     0       500  2.414510  0.598993    0.592634
   161     1       750  1.613083
   232     1      1000  1.393736  0.702875    0.686105
   277     2      1250  1.301472
   348     2      1500  1.318759  0.749684    0.729818
   393     2      1750  1.104028
   464     3      2000  0.944249  0.781448    0.753255
   509     3      2250  1.060442
   580     4      2500  1.044556  0.803831    0.771298
   625     4      2750  1.029417
   696     5      3000  0.803604  0.820800    0.779297
   742     5      3250  0.930568
   812     5      3500  1.010018  0.836517    0.787295
   858     6      3750  0.815485
   928     6      4000  0.788628  0.849059    0.797247
   974     7      4250  0.640736
  1045     7      4500  0.633681  0.860245    0.800037
  1090     8      4750  0.730090
  1161     8      5000  0.527101  0.868136    0.800316
  1206     8      5250  0.700827
  1276     9      5500  0.467462  0.881666    0.806734
  1322     9      5750  0.459065
  1393    10      6000  0.249392  0.890651    0.808408
  1438    10      6250  0.545176
  1509    10      6500  0.432544  0.899096    0.809338
  1554    11      6750  0.442327
  1625    11      7000  0.323499  0.904340    0.809896
  1671    12      7250  0.250669
  1742    12      7500  0.342897  0.913562    0.808873
  1787    13      7750  0.318918
  1858    13      8000  0.393423  0.919464    0.809617
  1903    13      8250  0.356062
  1974    14      8500  0.345688  0.928686    0.805711
  2019    14      8750  0.397935
  2090    15      9000  0.489637  0.932823    0.809710
  2135    15      9250  0.377421
  2206    16      9500  0.322918  0.944298    0.807943
  2251    16      9750  0.263796
  2322    16     10000  0.343181  0.947394    0.809989
  2367    17     10250  0.145322
  2438    17     10500  0.192195  0.952229    0.806269
  2484    18     10750  0.170917
  2555    18     11000  0.251535  0.957657    0.808966
  2600    18     11250  0.261610
  2670    19     11500  0.241796  0.961096    0.805618
  2715    19     11750  0.274856
  2786    20     12000  0.240116  0.962294    0.805060
  2831    20     12250  0.128955
  2902    21     12500  0.135354  0.967538    0.805432
  2948    21     12750  0.206794
  3019    21     13000  0.145498  0.971332    0.809245
  3064    22     13250  0.129848
  3135    22     13500  0.113590  0.974086    0.802827
  3180    23     13750  0.202485
  3251    23     14000  0.148311  0.978552    0.801246
  3296    24     14250  0.127158
  3367    24     14500  0.136398  0.979922    0.803478
  3411    24     14750  0.157498
  3482    25     15000  0.170290  0.980133    0.801153
  3528    25     15250  0.100260
  3598    26     15500  0.054467  0.983716    0.797805
  3643    26     15750  0.201115
  3714    26     16000  0.090699  0.985245    0.799479
  3759    27     16250  0.134497
  3830    27     16500  0.139638  0.987484    0.798270
  3875    28     16750  0.072080
  3946    28     17000  0.058888  0.988367    0.796224
  3991    29     17250  0.060461
  4062    29     17500  0.074158  0.988341    0.797340
  4108    29     17750  0.078021
  4178    30     18000  0.079720  0.990686    0.801153
  4224    30     18250  0.157003
  4294    31     18500  0.099798  0.991292    0.797991
  4339    31     18750  0.086782
  4410    32     19000  0.044016  0.992095    0.795852
  4455    32     19250  0.089855
  4526    32     19500  0.067208  0.993927    0.801525
  4571    33     19750  0.042091
  4642    33     20000  0.061792  0.993479    0.800316
  4687    34     20250  0.047824
  4757    34     20500  0.028520  0.993031    0.798084
  4803    34     20750  0.033756
  4873    35     21000  0.028373  0.994348    0.800502
  4919    35     21250  0.037591
  4989    36     21500  0.049615  0.995007    0.796038
  5034    36     21750  0.066788
  5105    37     22000  0.030910  0.994506    0.797247
  5150    37     22250  0.036549
  5220    37     22500  0.039228  0.996298    0.794643
  5265    38     22750  0.041883
  5336    38     23000  0.056682  0.995600    0.789156
  5381    39     23250  0.036092
  5452    39     23500  0.038123  0.995402    0.791295
  5497    40     23750  0.066151
  5568    40     24000  0.009139  0.996298    0.796131
  5613    40     24250  0.015870
  5684    41     24500  0.040400  0.996219    0.794364
  5729    41     24750  0.091437
  5799    42     25000  0.016112  0.996864    0.790737
  5844    42     25250  0.044739
  5915    43     25500  0.033106  0.996812    0.788783
  5960    43     25750  0.016160
  6031    43     26000  0.059396  0.997233    0.798456
  6075    44     26250  0.036884
  6146    44     26500  0.025723  0.997470    0.793527
  6191    45     26750  0.011331
  6262    45     27000  0.030124  0.997352    0.795108
  6307    45     27250  0.025396
  6377    46     27500  0.019299  0.997233    0.792132
  6422    46     27750  0.011597
  6492    47     28000  0.016864  0.997405    0.794457
  6537    47     28250  0.047743
  6608    48     28500  0.015101  0.997405    0.792969
  6653    48     28750  0.014463
  6724    48     29000  0.007408  0.997141    0.795294
  6769    49     29250  0.026949
  6839    49     29500  0.006758  0.997523    0.794643
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6914964725609876, cuda=True, d_embedding=300, d_hidden=582, d_out=1837, dev_every=500, dropout_prob=0.5176041728536023, epochs=50, gpu=0, log_every=250, lr=7.071016542004293e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.564722
    36     0       250  2.442750
    90     0       500  2.114065  0.674444    0.662388
   125     1       750  1.424867
   180     1      1000  1.483011  0.750000    0.728609
   215     2      1250  1.197269
   270     2      1500  1.030709  0.794754    0.767671
   305     2      1750  0.915858
   359     3      2000  0.841981  0.821801    0.783389
   394     3      2250  0.689253
   449     4      2500  0.827540  0.840983    0.792318
   484     4      2750  0.647144
   539     5      3000  0.624090  0.859731    0.798084
   574     5      3250  0.701672
   629     5      3500  0.607807  0.870626    0.805060
   664     6      3750  0.664170
   718     6      4000  0.484358  0.885421    0.804781
   753     7      4250  0.457020
   807     7      4500  0.477702  0.896817    0.808780
   843     8      4750  0.626717
   897     8      5000  0.370427  0.907831    0.808222
   932     8      5250  0.343653
   987     9      5500  0.384421  0.918147    0.803292
  1022     9      5750  0.384684
  1076    10      6000  0.306834  0.928554    0.807850
  1111    10      6250  0.283717
  1165    10      6500  0.319727  0.938765    0.805339
  1200    11      6750  0.299833
  1254    11      7000  0.314795  0.944403    0.804315
  1289    12      7250  0.221490
  1344    12      7500  0.264624  0.954074    0.802455
  1378    13      7750  0.206872
  1433    13      8000  0.256782  0.961096    0.800595
  1468    13      8250  0.273209
  1522    14      8500  0.149016  0.965601    0.798456
  1557    14      8750  0.152971
  1611    15      9000  0.102187  0.968249    0.796038
  1646    15      9250  0.119157
  1701    16      9500  0.187574  0.973664    0.800037
  1736    16      9750  0.161983
  1790    16     10000  0.097500  0.975232    0.798084
  1825    17     10250  0.149505
  1879    17     10500  0.134020  0.980581    0.798642
  1914    18     10750  0.079965
  1968    18     11000  0.170743  0.981595    0.790272
  2003    18     11250  0.250456
  2058    19     11500  0.045092  0.983242    0.792783
  2093    19     11750  0.049344
  2147    20     12000  0.101007  0.986035    0.794085
  2182    20     12250  0.115762
  2236    21     12500  0.048797  0.983808    0.788225
  2271    21     12750  0.053679
  2326    21     13000  0.072257  0.990514    0.792504
  2361    22     13250  0.102122
  2415    22     13500  0.049013  0.991041    0.797247
  2450    23     13750  0.058842
  2504    23     14000  0.095846  0.991107    0.786551
  2539    24     14250  0.021827
  2593    24     14500  0.039425  0.991871    0.794829
  2628    24     14750  0.078409
  2682    25     15000  0.051082  0.992543    0.783296
  2717    25     15250  0.037846
  2771    26     15500  0.056791  0.993742    0.791574
  2806    26     15750  0.030700
  2860    26     16000  0.063676  0.995284    0.788969
  2895    27     16250  0.031681
  2950    27     16500  0.018832  0.995007    0.787016
  2984    28     16750  0.033880
  3039    28     17000  0.034452  0.994216    0.785249
  3074    29     17250  0.018525
  3128    29     17500  0.015880  0.994770    0.787016
  3163    29     17750  0.041482
  3218    30     18000  0.012715  0.995929    0.786551
  3253    30     18250  0.044049
  3307    31     18500  0.033733  0.995046    0.783947
  3342    31     18750  0.017455
  3396    32     19000  0.013540  0.996614    0.790644
  3431    32     19250  0.029548
  3486    32     19500  0.008326  0.996219    0.788690
  3520    33     19750  0.020825
  3575    33     20000  0.036643  0.995969    0.784598
  3609    34     20250  0.019654
  3664    34     20500  0.009959  0.996706    0.784598
  3698    34     20750  0.033547
  3753    35     21000  0.003569  0.996759    0.786272
  3787    35     21250  0.018054
  3842    36     21500  0.013127  0.997088    0.784040
  3876    36     21750  0.009811
  3931    37     22000  0.053314  0.997115    0.785714
  3965    37     22250  0.008858
  4019    37     22500  0.071108  0.996930    0.783761
  4054    38     22750  0.055649
  4108    38     23000  0.013819  0.996838    0.790179
  4142    39     23250  0.027998
  4196    39     23500  0.040610  0.997009    0.786737
  4231    40     23750  0.034879
  4285    40     24000  0.016567  0.997233    0.785714
  4319    40     24250  0.097404
  4374    41     24500  0.014031  0.997036    0.783017
  4408    41     24750  0.005834
  4462    42     25000  0.009917  0.997470    0.784598
  4496    42     25250  0.014271
  4551    43     25500  0.018445  0.997550    0.782831
  4585    43     25750  0.008943
  4639    43     26000  0.003904  0.997457    0.787853
  4674    44     26250  0.005886
  4728    44     26500  0.012756  0.997694    0.784598
  4762    45     26750  0.008692
  4817    45     27000  0.006099  0.997826    0.780320
  4851    45     27250  0.003856
  4905    46     27500  0.002145  0.998156    0.785900
  4940    46     27750  0.043967
  4993    47     28000  0.024156  0.997708    0.783017
  5028    47     28250  0.011892
  5082    48     28500  0.011639  0.997774    0.783017
  5117    48     28750  0.005514
  5171    48     29000  0.020837  0.997984    0.783947
  5205    49     29250  0.012901
  5259    49     29500  0.004439  0.997023    0.784040
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6981598519502789, cuda=True, d_embedding=300, d_hidden=572, d_out=1837, dev_every=500, dropout_prob=0.573365060435407, epochs=50, gpu=0, log_every=250, lr=2.296303524621088e-05, n_directions=2, n_layers=5, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.567292
    45     0       250  3.862748
   116     0       500  2.893815  0.500870    0.502697
   162     1       750  2.209768
   233     1      1000  1.930265  0.623340    0.613467
   278     2      1250  1.659972
   349     2      1500  1.843833  0.685287    0.672061
   395     2      1750  1.496518
   466     3      2000  1.278484  0.721293    0.703776
   511     3      2250  1.227597
   582     4      2500  1.127993  0.749539    0.726376
   628     4      2750  1.107540
   698     5      3000  1.017912  0.768853    0.745164
   744     5      3250  1.347163
   815     5      3500  1.232441  0.786573    0.758929
   860     6      3750  1.170269
   930     6      4000  1.431991  0.799470    0.768415
   976     7      4250  0.966733
  1047     7      4500  0.561024  0.813593    0.777623
  1092     8      4750  0.757845
  1163     8      5000  0.786457  0.823474    0.782087
  1209     8      5250  1.041071
  1279     9      5500  0.834043  0.833619    0.789807
  1325     9      5750  0.841613
  1396    10      6000  0.654017  0.842301    0.792876
  1441    10      6250  0.868970
  1512    10      6500  0.813362  0.851589    0.796224
  1557    11      6750  0.730683
  1628    11      7000  0.848534  0.858664    0.799572
  1674    12      7250  0.665074
  1745    12      7500  0.660283  0.862273    0.801432
  1790    13      7750  0.400948
  1861    13      8000  0.540143  0.871838    0.804036
  1906    13      8250  0.586558
  1977    14      8500  0.590763  0.878201    0.807292
  2022    14      8750  0.502607
  2093    15      9000  0.827931  0.884538    0.808408
  2138    15      9250  0.636894
  2209    16      9500  0.639811  0.889136    0.808036
  2254    16      9750  0.363515
  2325    16     10000  0.459235  0.896554    0.811012
  2371    17     10250  0.331510
  2441    17     10500  0.569438  0.901349    0.813058
  2487    18     10750  0.492171
  2558    18     11000  0.451704  0.907185    0.811756
  2603    18     11250  0.244266
  2674    19     11500  0.326257  0.910334    0.815569
  2719    19     11750  0.384788
  2790    20     12000  0.285134  0.915709    0.814081
  2835    20     12250  0.400533
  2906    21     12500  0.270265  0.920162    0.813244
  2951    21     12750  0.437723
  3022    21     13000  0.414560  0.925959    0.815755
  3068    22     13250  0.299445
  3138    22     13500  0.370655  0.931308    0.814453
  3183    23     13750  0.309715
  3254    23     14000  0.257116  0.934325    0.814081
  3299    24     14250  0.210563
  3370    24     14500  0.341639  0.937961    0.813430
  3415    24     14750  0.232432
  3486    25     15000  0.302666  0.941466    0.812407
  3531    25     15250  0.244614
  3601    26     15500  0.189811  0.945510    0.810640
  3646    26     15750  0.170626
  3717    26     16000  0.190989  0.949858    0.811942
  3762    27     16250  0.174930
  3832    27     16500  0.209077  0.952611    0.813244
  3877    28     16750  0.223362
  3948    28     17000  0.202885  0.954680    0.810919
  3993    29     17250  0.139883
  4063    29     17500  0.201142  0.959528    0.808594
  4108    29     17750  0.208968
  4179    30     18000  0.191131  0.961346    0.808780
  4224    30     18250  0.202262
  4295    31     18500  0.190598  0.963572    0.807943
  4340    31     18750  0.168215
  4411    32     19000  0.157618  0.965298    0.807757
  4456    32     19250  0.231055
  4527    32     19500  0.116417  0.969725    0.809338
  4571    33     19750  0.151911
  4643    33     20000  0.115568  0.971411    0.807385
  4688    34     20250  0.157018
  4758    34     20500  0.167667  0.974138    0.807106
  4804    34     20750  0.096001
  4874    35     21000  0.176392  0.975640    0.804315
  4919    35     21250  0.156018
  4990    36     21500  0.114912  0.977274    0.803199
  5035    36     21750  0.169459
  5106    37     22000  0.132967  0.978446    0.806827
  5151    37     22250  0.185389
  5222    37     22500  0.099085  0.983216    0.802827
  5267    38     22750  0.093734
  5338    38     23000  0.137807  0.982952    0.804408
  5383    39     23250  0.106708
  5454    39     23500  0.142557  0.984177    0.806176
  5499    40     23750  0.094225
  5569    40     24000  0.144318  0.985903    0.804594
  5614    40     24250  0.100104
  5685    41     24500  0.132060  0.986114    0.805990
  5730    41     24750  0.115225
  5801    42     25000  0.125527  0.985640    0.803571
  5846    42     25250  0.181636
  5916    43     25500  0.109048  0.988723    0.801246
  5961    43     25750  0.074028
  6032    43     26000  0.120287  0.989856    0.805525
  6077    44     26250  0.083397
  6148    44     26500  0.141250  0.990277    0.801711
  6193    45     26750  0.064604
  6263    45     27000  0.058232  0.990936    0.802455
  6309    45     27250  0.074163
  6380    46     27500  0.113903  0.991278    0.802176
  6425    46     27750  0.077935
  6495    47     28000  0.060242  0.991542    0.802362
  6540    47     28250  0.069226
  6612    48     28500  0.067891  0.992938    0.801246
  6657    48     28750  0.064646
  6727    48     29000  0.049187  0.993452    0.801060
  6772    49     29250  0.059936
  6843    49     29500  0.046643  0.993017    0.800316
vocab size = 62622
num classes = 1837
unk vocab count = 22761
loading train/val/test datasets...
train_file: data/SimpleQuestions_v2/annotated_fb_data_train.txt, num train = 75910
val_file: data/SimpleQuestions_v2/annotated_fb_data_valid.txt, num dev   = 10845
test_file: data/SimpleQuestions_v2/annotated_fb_data_test.txt, num test  = 21687
Namespace(batch_size=128, birnn=True, clip=0.6457826196927045, cuda=True, d_embedding=300, d_hidden=595, d_out=1837, dev_every=500, dropout_prob=0.5425050131131591, epochs=50, gpu=0, log_every=250, lr=8.86800043737207e-05, n_directions=2, n_layers=4, resume_snapshot='', save_every=1000, save_path='saved_checkpoints', seed=1111, test=False, vocab_size=62622)
  Time Epoch Iteration     Loss   Train/Acc.   Val/Acc.
     1     0         1  7.506397
    36     0       250  2.245331
    92     0       500  1.663733  0.685721    0.671038
   128     1       750  1.563808
   185     1      1000  0.866703  0.764953    0.739211
   221     2      1250  0.945274
   277     2      1500  0.999025  0.803107    0.768787
   313     2      1750  0.662318
   368     3      2000  0.759074  0.829298    0.783761
   404     3      2250  0.934672
   461     4      2500  0.639517  0.850535    0.794922
   497     4      2750  0.624772
   553     5      3000  0.543281  0.866463    0.799386
   589     5      3250  0.646270
   645     5      3500  0.543542  0.880191    0.798549
   681     6      3750  0.596068
   737     6      4000  0.537042  0.894854    0.803013
   774     7      4250  0.325671
   830     7      4500  0.374512  0.906632    0.803757
   866     8      4750  0.367656
   922     8      5000  0.462438  0.915551    0.801804
   958     8      5250  0.265446
  1013     9      5500  0.345950  0.928186    0.803943
  1050     9      5750  0.477176
  1106    10      6000  0.316200  0.934444    0.802641
  1141    10      6250  0.293524
  1197    10      6500  0.360819  0.947196    0.801804
  1233    11      6750  0.198621
  1289    11      7000  0.209602  0.954232    0.798921
  1325    12      7250  0.170241
  1381    12      7500  0.134401  0.961096    0.800874
  1417    13      7750  0.184035
  1473    13      8000  0.124681  0.965694    0.798828
  1509    13      8250  0.269910
  1565    14      8500  0.142634  0.972544    0.791574
  1601    14      8750  0.174357
  1657    15      9000  0.104517  0.974823    0.792876
  1693    15      9250  0.215649
  1749    16      9500  0.100546  0.978354    0.797433
  1785    16      9750  0.097092
  1841    16     10000  0.084155  0.982675    0.797898
  1876    17     10250  0.054625
  1932    17     10500  0.135752  0.983637    0.792690
  1968    18     10750  0.096598
  2024    18     11000  0.119337  0.986891    0.791760
  2060    18     11250  0.033786
  2116    19     11500  0.104102  0.988143    0.786272
  2152    19     11750  0.057988
  2208    20     12000  0.099658  0.988749    0.787760
  2243    20     12250  0.062458
  2300    21     12500  0.066747  0.990198    0.787202
  2335    21     12750  0.023419
  2391    21     13000  0.121765  0.991779    0.783389
  2427    22     13250  0.115691
  2483    22     13500  0.075810  0.991463    0.782738
  2519    23     13750  0.051755
  2575    23     14000  0.027541  0.991423    0.789807
  2610    24     14250  0.031513
  2667    24     14500  0.050551  0.993202    0.792411
  2702    24     14750  0.023968
  2758    25     15000  0.019530  0.993689    0.783482
  2794    25     15250  0.033364
  2850    26     15500  0.014098  0.995218    0.785714
  2886    26     15750  0.051719
  2941    26     16000  0.061131  0.994967    0.786365
  2977    27     16250  0.058668
  3033    27     16500  0.017706  0.994151    0.784877
  3069    28     16750  0.060601
  3125    28     17000  0.068193  0.994374    0.787946
  3160    29     17250  0.028158
  3216    29     17500  0.032566  0.995033    0.779111
  3252    29     17750  0.081274
  3307    30     18000  0.067118  0.995797    0.787760
  3343    30     18250  0.026579
  3399    31     18500  0.022969  0.995349    0.786923
  3434    31     18750  0.041569
  3490    32     19000  0.019656  0.996048    0.785156
  3525    32     19250  0.005988
  3581    32     19500  0.021201  0.996575    0.783482
  3616    33     19750  0.014337
  3672    33     20000  0.086468  0.996061    0.782273
  3708    34     20250  0.121893
  3764    34     20500  0.006753  0.996272    0.782552
  3799    34     20750  0.021444
  3855    35     21000  0.049218  0.996390    0.781808
  3891    35     21250  0.013733
  3946    36     21500  0.008232  0.997563    0.784784
  3982    36     21750  0.008630
  4038    37     22000  0.030826  0.996904    0.782831
  4073    37     22250  0.014490
  4129    37     22500  0.005320  0.996351    0.783389
  4164    38     22750  0.004398
  4220    38     23000  0.004761  0.997563    0.786737
  4255    39     23250  0.034389
  4311    39     23500  0.025578  0.997233    0.783389
  4347    40     23750  0.081715
  4403    40     24000  0.007312  0.997352    0.783482
  4438    40     24250  0.003619
  4494    41     24500  0.005656  0.996878    0.783947
  4529    41     24750  0.008367
  4585    42     25000  0.076809  0.997352    0.779483
  4621    42     25250  0.048328
  4676    43     25500  0.004684  0.997523    0.780320
  4712    43     25750  0.007915
  4768    43     26000  0.007839  0.997536    0.785435
  4803    44     26250  0.003973
  4859    44     26500  0.005992  0.997813    0.785063
  4894    45     26750  0.081516
  4949    45     27000  0.006058  0.997905    0.785342
  4985    45     27250  0.006358
  5041    46     27500  0.044328  0.998037    0.785063
  5077    46     27750  0.034288
  5132    47     28000  0.012345  0.997602    0.783947
  5168    47     28250  0.005002
  5223    48     28500  0.001854  0.997694    0.785993
  5258    48     28750  0.002600
  5314    48     29000  0.002359  0.997681    0.785993
  5350    49     29250  0.001417
  5406    49     29500  0.001608  0.998261    0.787016
