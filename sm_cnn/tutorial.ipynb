{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM Model PyTorch Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This purpose of this notebook is to explain how to use PyTorch to implement the SM Model for new PyTorch users. Here are the recommended prerequisites before reading this walkthrough:\n",
    "\n",
    "* Have knowledge of Convolutional Neural Networks. If not these are helpful slides: https://cs.uwaterloo.ca/~mli/Deep-Learning-2017-Lecture5CNN.ppt.\n",
    "* Read the SM Model paper: http://dl.acm.org/citation.cfm?id=2767738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    \n",
    "        def __init__(self, input_n_dim, filter_width, \\\n",
    "            conv_filters=100, no_ext_feats=False, ext_feats_size=4, n_classes=2):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.no_ext_feats = no_ext_feats\n",
    "\n",
    "        self.conv_channels = conv_filters\n",
    "        n_hidden = 2*self.conv_channels + (0 if no_ext_feats else ext_feats_size)\n",
    "\n",
    "        self.conv_q = nn.Sequential(\n",
    "            nn.Conv1d(input_n_dim, self.conv_channels, filter_width, padding=filter_width-1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv1d(input_n_dim, self.conv_channels, filter_width, padding=filter_width-1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.combined_feature_vector = nn.Linear(2*self.conv_channels + \\\n",
    "            (0 if no_ext_feats else ext_feats_size), n_hidden)\n",
    "\n",
    "        self.combined_features_activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "\n",
    "    def forward(self, question, answer, ext_feats):\n",
    "        q = self.conv_q.forward(question)\n",
    "        q = F.max_pool1d(q, q.size()[2])\n",
    "        q = q.view(-1, self.conv_channels)\n",
    "\n",
    "        a = self.conv_a.forward(answer)\n",
    "        a = F.max_pool1d(a, a.size()[2])\n",
    "        a = a.view(-1, self.conv_channels)\n",
    "\n",
    "        x = None\n",
    "        if self.no_ext_feats:\n",
    "            x = torch.cat([q, a], 1)\n",
    "        else:\n",
    "            x = torch.cat([q, a, ext_feats], 1)\n",
    "\n",
    "        x = self.combined_feature_vector.forward(x)\n",
    "        x = self.combined_features_activation.forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
