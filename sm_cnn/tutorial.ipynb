{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM CNN Model PyTorch Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This purpose of this notebook is to explain how to use PyTorch to implement the SM CNN Model for new PyTorch users. Here are the recommended prerequisites before reading this walkthrough:\n",
    "\n",
    "* Have knowledge of Convolutional Neural Networks. If not these are helpful slides: https://cs.uwaterloo.ca/~mli/Deep-Learning-2017-Lecture5CNN.ppt.\n",
    "* Read the SM Model paper: http://dl.acm.org/citation.cfm?id=2767738\n",
    "\n",
    "The following is a slightly modified version of the SM CNN architecture presented in the paper that will be implemented in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/nn-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below we define the model, with detailed explanations in comments. This model is slightly different from the model in model.py to keep the tutorial straightforward (e.g. ignore GPU code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    \"\"\"\n",
    "    All PyTorch models should subclass nn.Module, the base class for neural network modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_n_dim, filter_width, conv_filters=100,\n",
    "                 no_ext_feats=False, ext_feats_size=4, n_classes=2):\n",
    "        \"\"\"\n",
    "        :param input_n_dim: the dimension of each word vector\n",
    "        :param filter_width: the width of each convolution filter\n",
    "        :param conv_filters: the number of convolution filters\n",
    "        :param no_ext_feats: whether or not to use additional features X_feat\n",
    "        :param ext_feats_size: number of external features to use\n",
    "        :param n_classes: number of label classes\n",
    "        \"\"\"\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.no_ext_feats = no_ext_feats\n",
    "\n",
    "        # self.conv_channels specify the dimension of the output of the convolution,\n",
    "        # i.e. the number of convolution feature maps\n",
    "        self.conv_channels = conv_filters\n",
    "        # the elements in the hidden layer consist of equal number of inputs from the query and document (hence the 2*)\n",
    "        # and optionally the additional features (ext_feats_size)\n",
    "        n_hidden = 2*self.conv_channels + (0 if no_ext_feats else ext_feats_size)\n",
    "\n",
    "        # define the convolution for the question/query - 1D convolution followed by tanh nonlinear activation\n",
    "        self.conv_q = nn.Sequential(\n",
    "            # the first parameter specifies the input dimension, the second parameter specifies the output dimension\n",
    "            nn.Conv1d(input_n_dim, self.conv_channels, filter_width, padding=filter_width-1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # define the convolution for the answer/document\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv1d(input_n_dim, self.conv_channels, filter_width, padding=filter_width-1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # combining the features from the question, answer, and external features if any into a single vector\n",
    "        # note PyTorch nn classes follow a similar signature - the first parameter specifies the input dimension,\n",
    "        # the second parameter specifies the output dimension\n",
    "        self.combined_feature_vector = nn.Linear(2*self.conv_channels + \\\n",
    "            (0 if no_ext_feats else ext_feats_size), n_hidden)\n",
    "\n",
    "        # defining other layers used in the network, note they are not yet linked with each other yet\n",
    "        self.combined_features_activation = nn.Tanh()\n",
    "        # dropout is used to prevent overfitting and only used during training\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.hidden = nn.Linear(n_hidden, n_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "\n",
    "\n",
    "    def forward(self, question, answer, ext_feats):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        The question and answer tensors are 3-dimensional. The first dimension specifies the sentence - it\n",
    "        can be larger than 1 since multiple sentences can be batched together in one forward pass.\n",
    "        The second and third dimensions specify the dimension of the word vector and the number of tokens respectively.\n",
    "        \n",
    "        :param question: the sentence matrices of questions (queries). Note the plural form - this is explained above.\n",
    "        :param answer: the sentence matrices of answers (documents). Note the plural form - this is explained above.\n",
    "        :param ext_feats: the external features for the question-answer pairs.\n",
    "        :returns: the log-likelihood of the question-answer pairs belonging in each class.\n",
    "        \"\"\"\n",
    "        # feed the question sentence matrices through the conv_q layers.\n",
    "        # IMPORTANT: the second dimension of the question MUST match the the first argument\n",
    "        # the Conv1d instance created (input_n_dim). The first dimension of the question specifies\n",
    "        # the batch size (number of questions).\n",
    "        q = self.conv_q.forward(question)\n",
    "        # max pool using q.size()[2] as the window size, which is the length of each convolution feature map\n",
    "        q = F.max_pool1d(q, q.size()[2])\n",
    "        # reshape max pooled elements into a vector of length equal to the number of feature maps\n",
    "        # the max pooling takes one value (the max) out of each convolution feature map\n",
    "        q = q.view(-1, self.conv_channels)\n",
    "\n",
    "        # feed the answer sentence matrices through the conv_a layers, similar to the previous part for the question.\n",
    "        a = self.conv_a.forward(answer)\n",
    "        a = F.max_pool1d(a, a.size()[2])\n",
    "        a = a.view(-1, self.conv_channels)\n",
    "\n",
    "        # concatenate the outputs of the conv_q, conv_a layers together\n",
    "        # with optionally the ext_feats along the first dimension\n",
    "        x = None\n",
    "        if self.no_ext_feats:\n",
    "            x = torch.cat([q, a], 1)\n",
    "        else:\n",
    "            x = torch.cat([q, a, ext_feats], 1)\n",
    "\n",
    "        # feed the concatenated feature vector through the rest of the network (starting with join layer in figure)\n",
    "        x = self.combined_feature_vector.forward(x)\n",
    "        x = self.combined_features_activation.forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden(x)\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(model_fname):\n",
    "        return torch.load(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load a pre-trained model with one input and see what the model actually does. For this, you'll need to clone the `data` and `models` projects in https://github.com/castorini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains some bootstrapping code to prepare the data, load the model, etc.. It is not important to understand it just to see how the SM CNN model itself works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - WARNING: expecting a .gz file. Is the ../../data/word2vec/aquaint+wiki.txt.gz.ndim=50.bin in the correct             format?\n",
      "/Users/michael/anaconda/lib/python3.6/site-packages/torch/serialization.py:284: SourceChangeWarning: source code of class 'model.QAModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from train import Trainer\n",
    "import utils\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# cache word embeddings\n",
    "word_vectors_file = '../../data/word2vec/aquaint+wiki.txt.gz.ndim=50.bin'\n",
    "cache_file = os.path.splitext(word_vectors_file)[0] + '.cache'\n",
    "utils.cache_word_embeddings(word_vectors_file, cache_file)\n",
    "\n",
    "vocab_size, vec_dim = utils.load_embedding_dimensions(cache_file)\n",
    "\n",
    "trained_model = QAModel.load('../../models/sm_model/sm_model.TrecQA.TRAIN-ALL.2017-04-02.castor')\n",
    "trained_model_weights = list(trained_model.parameters())\n",
    "evaluator = Trainer(trained_model, 0.001, 0.0, False, vec_dim)\n",
    "\n",
    "evaluator.load_input_data('../../data/TrecQA', cache_file, None, None, 'raw-dev')\n",
    "\n",
    "questions, sentences, labels, maxlen_q, maxlen_s, ext_feats = evaluator.data_splits['raw-dev']\n",
    "word_vectors = evaluator.embeddings\n",
    "batch_inputs, batch_labels = evaluator.get_tensorized_inputs(\n",
    "    questions[0:1],\n",
    "    sentences[0:1],\n",
    "    labels[0:1],\n",
    "    ext_feats[0:1],\n",
    "    word_vectors, vec_dim\n",
    ")\n",
    "\n",
    "xq, xa, x_ext_feats = batch_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The question we want to compute similarity for and its sentence matrix is shown below. Each column represents the word vector for the corresponding word/token in the sentence (9 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what ethnic group / race are crip members ?\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0151  0.0496 -0.0434 -0.5739  0.2806  0.0424  0.2552  0.3129  0.3887\n",
      "  0.3431  0.2241 -0.0209 -0.7470 -0.0753  0.4269 -0.0267  0.1465 -0.7898\n",
      " -0.1953 -0.4663 -0.3478  0.7483  0.2263 -0.3437 -0.3074 -0.3011 -0.0599\n",
      "  0.0161 -0.2898 -0.2497  0.2319 -0.1071 -0.1151 -0.1103 -0.4594  0.8386\n",
      " -0.6012 -0.3394 -0.1846 -0.6559 -0.3885 -0.0970 -0.0578 -0.3267 -0.4596\n",
      " -0.1988  0.2961 -0.0193 -0.4824 -0.0110 -0.1166 -0.2493  0.0772 -0.0718\n",
      "  0.0492  0.0412  0.0854 -0.3839  0.0192 -0.1304 -0.2810  0.1978  0.0469\n",
      " -0.0640 -0.0597  0.0677 -0.0215  0.1749  0.0468  0.0171  0.1196  0.6734\n",
      " -0.0178 -0.3511 -0.1803  0.3700  0.0912 -0.1384  0.0203 -0.3293  0.2773\n",
      " -0.1661 -0.5140 -0.3630 -0.4209 -0.1508  0.2184 -0.2679 -0.7618 -0.0724\n",
      " -0.1613  0.4295  0.1512  0.5132 -0.2926 -0.1213  0.1927  0.1640  1.5271\n",
      "  0.0671 -0.1575 -0.2356  0.4252 -0.5078 -0.1085 -0.5529  0.1896 -0.4746\n",
      " -0.3540 -0.0366  0.0548  0.1675  0.2479 -0.0018 -0.4409 -0.0313 -0.2920\n",
      "  0.0007 -0.2413  0.0105 -0.6915 -0.0720 -0.1719 -0.0384  0.0463 -0.1074\n",
      " -0.0079 -0.3216 -0.3839  0.5540 -0.5102  0.1859  0.3327 -0.3655  0.7154\n",
      " -0.1233 -0.0394 -0.0246  0.6931 -0.1356  0.0940  0.4797  0.1499  0.0887\n",
      " -0.6274 -0.2394 -0.4204 -0.4998  0.0115 -0.2200 -0.3197 -0.1628 -0.3403\n",
      " -0.0655  0.0988  0.1530  0.0844 -0.1445  0.3213 -0.1660  0.0719  0.0086\n",
      " -0.3158 -0.1170 -0.4996  0.6942 -0.2527 -0.3549 -0.5263  0.0462  0.3434\n",
      "  0.1226 -0.0526  0.4190  0.0782  0.2516  0.0831 -0.0607  0.1898  0.1521\n",
      " -0.2113 -0.6138 -0.1357  0.6663 -0.0023 -0.1512 -0.1061 -0.2650  0.2952\n",
      " -0.0091 -0.2924  0.1389  0.0568  0.5159 -0.5317  0.0165  0.0666  0.0335\n",
      "  0.0375  0.0978  0.0299  0.1299 -0.3886 -0.1056  0.2029 -0.1294  0.3008\n",
      " -0.2792 -0.4193  0.0682 -0.3850  0.0936  0.0021 -0.1140  0.1280 -0.4693\n",
      " -0.2200  0.0723  0.2480  0.3454  0.9106 -0.0432  0.0209 -0.0957 -0.1656\n",
      " -0.1133  0.1281 -0.2189 -0.3610 -0.6643  0.2118  0.0837 -0.1434 -0.7695\n",
      "  0.0176  0.0174 -0.1320  0.1855  0.0459  0.1588 -0.2744 -0.1392 -0.0493\n",
      "  0.1774 -0.0294  0.3108 -0.1511 -0.0425 -0.2990 -0.2306  0.0126 -0.4989\n",
      " -0.1766  0.0025  0.0528  0.5702  0.2832 -0.1769  0.3303  0.3302 -0.0634\n",
      " -0.3175  0.3858  0.1501 -0.2329  0.2105  0.6620 -0.0749  0.1194 -0.0978\n",
      " -0.4202  0.2960 -0.3785  0.0142 -0.2628 -0.0856 -0.1099 -0.0249 -0.4249\n",
      " -0.0487  0.0886  0.1878  0.0852 -0.0407  0.0464  0.1496  0.2344 -1.1952\n",
      " -0.2744 -0.2275  0.0303 -0.3330 -0.2031 -0.0309 -0.2991 -0.0790  0.1272\n",
      "  0.0340 -0.2332 -0.5344 -0.2633 -0.1335 -0.3919  0.0757 -0.4951 -0.5789\n",
      " -0.4006 -0.6026 -0.0764  0.2737 -0.0955 -0.2100 -0.1652 -0.2111  0.1166\n",
      "  0.0501  0.0441  0.0138  0.4589 -0.4650  0.0754 -0.4981 -0.0956  0.1995\n",
      "  0.5694  0.0681 -0.1873 -0.1168 -0.0904 -0.0051 -0.0246  0.0997  0.5085\n",
      " -0.0913  0.1658 -0.1886  0.3961  0.5671 -0.3347 -0.0061 -0.3095  0.6921\n",
      "  0.2070  0.1439 -0.0818  0.4481  0.2961 -0.6117  0.1387 -0.4862  0.2324\n",
      "  0.0475 -0.4960 -0.0763  0.1677 -0.2085 -0.7096 -0.2205 -0.2063  0.5578\n",
      "  0.0095  0.0456  0.0955 -0.0850  0.4013  0.2275 -0.1590  0.2368  0.3003\n",
      " -0.5539 -0.1090 -0.1168 -0.0834 -0.0060 -0.3147 -0.1089 -0.1458 -0.5296\n",
      "  0.2658  0.2245  0.0411 -0.0014  0.2086 -0.0570  0.1433  0.1452 -0.1137\n",
      " -0.2537  0.1439  0.0684  0.1227 -0.8298  0.1437  0.2467  0.3032  0.0563\n",
      "  0.3145 -0.0750 -0.0387  0.0217  0.4454  0.1572 -0.1294 -0.1649  0.1815\n",
      "  0.1173 -0.4452 -0.2280 -0.0356 -0.0173 -0.1493  0.3712  0.1379  0.0715\n",
      " -0.0003  0.2991  0.1604 -0.7523  0.5310  0.1785 -0.0793  0.3466  0.3423\n",
      "  0.0574  0.2690 -0.1271  0.7327 -0.2957 -0.4391 -0.0719 -0.3435  0.3495\n",
      " -0.3799  0.2458 -0.1498  0.1490 -0.2352 -0.0072  0.3003  0.0068  0.7245\n",
      " -0.1597  0.6455  0.2640  0.2441  0.0821 -0.0372 -0.1428  0.3391 -0.0293\n",
      "[torch.FloatTensor of size 1x50x9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])\n",
    "print(xq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer we want to compute similarity for and its sentence matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prison gangs have a de facto negotiation system to defuse potential conflicts , black gang members said .\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 5 \n",
      "   2.5708e-01  3.8999e-01 -2.3996e-02  5.0112e-02  2.4804e-01  5.0402e-02\n",
      "  4.8300e-03 -1.0852e-01  3.0625e-01 -2.6673e-01 -3.4403e-01  3.0642e-01\n",
      " -9.3015e-02 -3.4896e-01 -4.5291e-01 -4.3777e-01  8.9621e-01  8.6556e-01\n",
      " -4.9385e-01 -5.1353e-01 -3.3375e-01  7.5828e-02  2.8502e-02 -3.9073e-01\n",
      " -1.5353e-01 -4.6417e-01 -2.4324e-01  3.2717e-01  3.2242e-01  1.5353e-02\n",
      " -3.0698e-02  2.0278e-01 -1.3350e-01 -1.2179e-01 -2.7394e-01 -2.4425e-01\n",
      "  1.5015e-01 -3.9379e-01 -3.0872e-03  3.4965e-01  6.4216e-02  5.7407e-02\n",
      " -1.3859e-01 -1.8899e-01 -3.3805e-02 -2.1679e-01  1.6352e-02 -2.0942e-01\n",
      " -4.4011e-02 -2.0412e-01 -1.2718e-02 -2.9922e-01 -2.2560e-01 -5.0424e-01\n",
      " -4.7677e-01 -2.1772e-01  1.6759e-01 -2.2158e-01  3.2218e-01 -2.8547e-01\n",
      "  1.9762e-01  1.8645e-01 -2.6878e-01  6.7237e-02  2.2902e-01  2.7136e-01\n",
      " -9.6843e-02 -2.8648e-01  4.8062e-02 -9.0392e-02 -3.9027e-01 -7.0064e-01\n",
      "  2.7142e-02 -1.1838e-01  6.7476e-03 -2.2742e-01 -8.3595e-02  8.9844e-02\n",
      " -2.8249e-01 -5.9309e-03 -2.3406e-01  8.8141e-02  1.6779e-01  2.9883e-01\n",
      " -1.9021e-02 -7.0202e-02  1.9174e-01  2.8111e-01 -6.6898e-01 -2.9242e-01\n",
      "  8.0912e-02  4.6823e-01  6.3128e-02  3.1050e-01  1.3500e-01  2.1025e-03\n",
      " -1.6557e-01 -3.4329e-02 -3.0178e-01  2.6118e-01 -5.7731e-02  5.4411e-02\n",
      " -2.7099e-02  7.1839e-02  2.5660e-01 -4.5657e-01  1.0452e-01 -1.6718e-01\n",
      " -7.9341e-02 -1.8500e-01 -4.5357e-01  5.6582e-02 -9.3592e-01 -3.6081e-01\n",
      " -3.7141e-01 -4.0973e-02  4.2250e-01 -4.3003e-01 -1.5349e-01  1.2085e-01\n",
      "  2.0538e-02 -1.8556e-01 -3.7597e-01  2.1234e-01  9.5635e-02 -1.6949e-01\n",
      " -8.3975e-02 -2.5372e-02 -2.6113e-01 -2.2607e-01 -2.5088e-01  3.5815e-03\n",
      "  2.2534e-01  1.2058e-01  6.3200e-03 -1.0127e-01  5.5227e-01  7.6757e-01\n",
      "  2.6321e-01 -4.8371e-01  2.1312e-01  2.4686e-01  3.7954e-01  1.4968e-01\n",
      "  6.1870e-02  3.8972e-01 -2.0496e-01  6.1326e-01 -1.9527e-01 -3.3493e-01\n",
      " -4.8968e-01  1.4020e-01  3.6751e-01 -1.8951e-01 -2.8107e-01 -4.2319e-01\n",
      "  1.0181e-01 -8.5203e-02  2.3895e-01  2.6896e-01 -5.7950e-01 -1.4189e-01\n",
      " -2.5329e-02 -3.8229e-04 -3.0407e-01 -5.2997e-02  1.2016e-01  3.6891e-01\n",
      " -1.8830e-01  1.5275e-01 -4.0709e-01  2.3962e-01 -3.4547e-01  8.7194e-02\n",
      "  2.6614e-01  5.7428e-01  1.7749e-01 -2.2663e-01 -5.1729e-02  2.9435e-01\n",
      "  3.0386e-01  2.6549e-01 -1.1983e-02  6.9022e-02 -2.8961e-01  2.4402e-01\n",
      "  3.2548e-01  1.8942e-01 -1.7551e-01 -5.5475e-02 -4.5593e-01 -3.3341e-01\n",
      "  3.2486e-02 -1.7770e-01  1.8353e-01 -2.3844e-02 -3.5443e-01 -3.0561e-01\n",
      "  3.3958e-02  4.3419e-02 -2.0782e-01 -1.4590e-01 -1.7216e-01 -1.2334e-02\n",
      " -4.7119e-01 -3.9033e-01 -2.7276e-01  4.5950e-01 -9.8142e-02 -1.7993e-01\n",
      " -2.8759e-01 -3.4055e-01  1.4657e-01  8.5024e-02  2.1647e-01  1.7953e-01\n",
      "  2.9148e-01  1.9445e-01  3.5277e-01  5.3860e-02  2.7412e-01  3.1807e-01\n",
      " -1.3329e-03 -7.3173e-02 -1.4319e-01  2.1526e-01  2.5969e-02  8.4222e-03\n",
      "  1.1191e-01 -5.2150e-02 -3.2841e-01 -7.4616e-02  1.4607e-01  1.3281e-02\n",
      "  2.3386e-02 -6.9558e-02 -4.0313e-01  3.0968e-01  2.2275e-01  6.1457e-01\n",
      "  2.5757e-01  3.3291e-02  1.6475e-01  1.3747e-01  5.8891e-01  3.8479e-01\n",
      "  1.5261e-02  1.4618e-01 -5.8200e-01  1.1893e-01  2.2807e-01  1.7256e-01\n",
      "  2.1948e-01  1.9270e-01  1.0026e-01  1.0854e-01 -1.6736e-01 -3.6372e-02\n",
      " -4.1369e-01  2.7624e-01 -8.1911e-03 -5.4528e-02  3.6473e-01  3.0353e-01\n",
      " -1.0920e-01 -2.1676e-01 -1.1025e-02  4.6923e-01 -3.0289e-01  1.2608e-01\n",
      "  1.3746e-01  7.8488e-02  1.0161e-01  3.8342e-01  2.9331e-02 -1.0748e-01\n",
      " -2.2067e-01  2.1949e-01  2.4732e-01 -5.5824e-02  3.7205e-01  5.9423e-01\n",
      "  2.1221e-02  2.5128e-01 -5.4153e-01  4.1714e-01 -1.2188e-01 -3.3004e-01\n",
      " -6.5652e-01 -2.8388e-01 -5.0218e-02 -7.8614e-02  3.2547e-01  4.9051e-01\n",
      " -1.8654e-01  1.8979e-02 -1.1940e-01 -3.7822e-01 -1.5405e-02  5.5930e-01\n",
      "\n",
      "Columns 6 to 11 \n",
      "  -1.7935e-01 -2.8863e-01  1.6573e-01 -3.7657e-02  2.1578e-02  2.7844e-02\n",
      " -3.2156e-02  1.5802e-02  1.2740e-01  4.2814e-02 -7.3171e-02  1.0815e-02\n",
      "  6.5720e-02 -1.5322e-02 -3.0233e-01 -2.6051e-01 -3.0197e-01 -1.8885e-01\n",
      " -3.0546e-01 -2.8784e-01 -1.7849e-01 -5.8222e-01 -2.8490e-01 -2.9580e-01\n",
      " -1.5457e-05  6.5783e-02 -6.9654e-02 -2.4238e-01 -4.3151e-02 -2.0093e-01\n",
      " -3.7269e-01  4.9492e-02  1.6792e-01 -4.5116e-01  5.7343e-02  1.3378e-01\n",
      "  6.9899e-02 -2.0877e-01  7.9100e-02 -2.0599e-01 -2.3319e-01 -1.3248e-01\n",
      "  3.7252e-02  8.5891e-02 -1.1350e-02 -1.6187e-01 -2.2093e-01 -3.8107e-03\n",
      " -1.3274e-01 -1.3454e-01 -2.0553e-03 -2.4901e-01 -3.3694e-01 -2.4716e-01\n",
      " -4.0949e-01 -1.3240e-01 -5.8299e-02 -2.8247e-02 -2.3955e-01 -3.3183e-01\n",
      "  1.1844e-01 -3.5996e-01 -2.5504e-01  4.2637e-01 -2.2351e-01  2.5501e-01\n",
      " -2.4188e-01 -2.6847e-01  2.2539e-02 -1.8521e-01 -2.5575e-01 -3.1832e-01\n",
      "  8.6078e-02  1.6239e-02 -6.7007e-02 -2.0579e-01 -2.6794e-01  1.2282e-01\n",
      "  1.5377e-01  9.9038e-02  3.8272e-01  4.8777e-01  1.4659e-01 -7.9518e-02\n",
      "  3.6736e-02  9.9906e-02 -2.7898e-01  6.7213e-02  1.0028e-01 -3.3567e-01\n",
      " -1.4361e-01 -1.7246e-01 -7.5085e-02  7.6083e-01  6.4354e-02 -3.5448e-02\n",
      "  2.0077e-02  2.9121e-02 -4.5482e-01 -1.3526e-01 -1.3849e-02  1.3050e-01\n",
      "  3.6063e-02 -5.5479e-02  4.4583e-03  2.1666e-01  3.1132e-02  2.7679e-01\n",
      "  2.7306e-01 -3.0010e-01 -1.3321e-02 -8.2539e-02 -1.4548e-01  1.2376e-02\n",
      "  3.0564e-01  1.8764e-01  8.0330e-02  3.6017e-01  5.1016e-01  3.0115e-01\n",
      " -2.8295e-01 -1.4109e-01 -2.0029e-01  1.6480e-01 -3.8884e-01 -5.2126e-01\n",
      " -1.4822e-01 -3.6047e-01 -6.1210e-02 -3.6284e-01 -2.8038e-01 -2.7591e-01\n",
      "  1.3310e-01  1.6588e-01  1.6619e-01  1.3513e-01 -1.6380e-01  2.2308e-01\n",
      " -1.8856e-01 -4.0084e-02 -1.7636e-02 -2.3818e-01 -2.8450e-02 -3.5268e-01\n",
      "  2.6586e-01  1.1809e-01 -1.1042e-02 -1.1026e-02  6.0313e-02  3.1582e-01\n",
      " -5.5881e-02 -2.8943e-01 -2.2661e-01 -1.7926e-03 -1.7484e-01  2.5241e-01\n",
      " -4.4362e-03  2.3366e-01 -2.4587e-02  2.7093e-01  3.0925e-01  2.9100e-01\n",
      "  1.5623e-01  1.2752e-01 -1.0377e-01 -1.3877e-01  7.9265e-02 -3.4404e-02\n",
      "  4.9732e-02 -4.2150e-02 -2.6230e-01 -3.2258e-01 -2.1005e-01  4.0918e-02\n",
      "  4.5204e-01  1.5548e-01  1.3125e-01 -4.2019e-02 -7.6171e-02  4.9684e-01\n",
      "  1.9614e-01 -1.0357e-01 -2.9348e-01  2.0737e-01 -6.6424e-02  2.6936e-01\n",
      "  3.9969e-01  4.7407e-01 -1.0327e-01 -5.5329e-02  2.8245e-01  2.4478e-01\n",
      " -4.7361e-03  3.1665e-02 -5.4271e-02 -7.3083e-03  6.3794e-02  4.9556e-02\n",
      " -1.9251e-01 -3.5522e-01 -1.5216e-01  5.9268e-01 -3.0530e-01  8.4574e-02\n",
      " -3.2272e-01 -1.5581e-01 -9.0195e-02 -4.5905e-01 -8.9387e-02 -4.9700e-01\n",
      "  1.2461e-01  2.6726e-02  2.0786e-01 -3.3925e-01  1.0245e-01  8.6226e-02\n",
      "  2.5372e-01  2.2420e-02  3.4960e-01  1.4467e-01  3.7122e-01  4.7216e-01\n",
      " -1.0338e-01 -3.5424e-01 -4.2881e-01 -1.6392e-01 -3.3738e-01  4.4763e-03\n",
      "  4.4715e-01  1.7815e-01 -1.8773e-02  8.0515e-01  1.4246e-01  2.1602e-01\n",
      "  3.4008e-01  1.8024e-01 -2.0780e-02  2.3970e-01  2.5983e-03  8.8234e-02\n",
      "  4.4620e-01  1.3289e-01  2.6864e-01  1.3388e-01  4.4242e-01  2.3248e-01\n",
      " -3.9997e-02 -3.5818e-01 -8.4228e-02 -3.1753e-01 -2.7761e-01  1.1070e-01\n",
      "  5.3184e-02  6.8694e-02  5.1910e-01  4.7471e-01  2.3476e-01  4.0092e-01\n",
      "  3.2886e-02 -1.9679e-01 -3.1732e-01 -2.8529e-01  9.8000e-02  2.7121e-01\n",
      "  2.5797e-01 -8.3053e-02  9.0434e-02 -1.8828e-01  4.5098e-02  2.8421e-01\n",
      " -2.3783e-01 -9.3363e-02 -2.6916e-01 -4.9593e-01 -3.3679e-01 -3.4789e-01\n",
      "  4.4160e-02 -1.8660e-01 -2.3476e-01  1.5067e-01  5.7155e-02  2.8010e-01\n",
      "  1.9837e-02 -2.1340e-01 -1.4558e-01 -1.3483e-01 -6.7499e-02  1.3453e-01\n",
      " -2.5572e-01 -6.6038e-02 -3.6775e-01  3.6257e-02 -3.1069e-01 -2.6344e-01\n",
      "  3.7432e-01  4.9239e-01  1.6311e-01 -1.8455e-01  1.9356e-01  2.7588e-01\n",
      "\n",
      "Columns 12 to 17 \n",
      "   1.9578e-01  1.5332e-01  1.4900e-01  3.1285e-01  3.9753e-03 -2.6769e-01\n",
      " -7.1270e-01  3.5821e-01 -7.9846e-02  1.4652e-01  1.2011e-01 -8.8015e-01\n",
      "  3.7198e-01 -3.8091e-02 -1.1683e-01 -3.0107e-01 -2.0617e-01  7.6408e-01\n",
      "  4.3086e-01  2.0971e-01 -3.8660e-01 -4.5940e-01 -3.8621e-01  2.1510e-01\n",
      " -1.1651e-01  1.6556e-02 -3.0306e-02 -3.2672e-01 -5.1940e-01  1.2445e-01\n",
      " -2.1542e-01 -9.0503e-02 -1.4463e-02  7.7189e-02 -4.2084e-01 -7.6729e-01\n",
      "  1.8048e-01 -3.1707e-01 -3.8199e-01  1.9779e-01  7.2794e-02  1.1392e-01\n",
      "  1.1416e-01 -2.4493e-01  2.1542e-01  1.1962e-01  1.6310e-01  4.1672e-01\n",
      " -4.0256e-02  2.1483e-01  1.8636e-02 -3.2929e-01  1.1115e-01  4.2814e-01\n",
      "  8.9727e-02 -2.0925e-01 -6.1722e-01 -7.6178e-01 -2.8169e-01 -1.2005e-01\n",
      "  1.0935e+00  3.7767e-01  3.5128e-01  1.6397e-01  1.7133e-03  8.2978e-01\n",
      "  5.3731e-02 -2.7635e-01 -3.0193e-01  1.8957e-01 -7.9951e-02  2.7663e-01\n",
      "  8.6486e-02 -1.3268e-01 -1.1090e-01 -3.1283e-02 -4.8572e-01 -1.4066e-01\n",
      " -1.8527e-01 -9.7157e-02 -1.2504e-01  4.6328e-02  3.6014e-01  1.1029e-01\n",
      "  4.6746e-01  1.0305e-01  8.3352e-02 -3.6549e-01 -6.3984e-02  1.2050e+00\n",
      "  2.6197e-01  1.3697e-01  5.1634e-01  1.4990e-01 -4.5807e-03  6.9788e-01\n",
      " -4.5049e-01 -2.1045e-01 -4.4173e-01 -1.6283e-01 -3.5296e-01 -1.0909e+00\n",
      "  3.4609e-03  2.3115e-01 -1.3514e-01  7.1922e-02  5.7036e-02 -1.3445e-01\n",
      "  1.7356e-01 -3.0711e-01 -3.2174e-01  4.6222e-02 -2.5287e-01  2.5339e-01\n",
      "  4.8128e-01 -2.7014e-01  3.8493e-02  1.8983e-01  2.1870e-01  5.5252e-01\n",
      " -2.9918e-02 -3.0915e-01 -1.2924e-01 -2.6501e-01 -3.2193e-01  7.2303e-01\n",
      "  2.8722e-01  1.3867e-01  1.5613e-01  6.6577e-02  1.9967e-01 -1.0405e+00\n",
      " -2.6427e-01 -8.1294e-02 -2.4166e-02 -1.2940e-01 -2.1231e-01  1.1781e-01\n",
      "  6.0241e-02 -9.2992e-02 -3.0297e-01  1.2797e-01  1.0785e-01 -1.4792e-01\n",
      " -2.3153e-01  3.0175e-01  2.4971e-02 -9.5658e-02 -2.1699e-01 -7.8862e-02\n",
      " -4.2177e-01  2.8894e-01 -1.0432e-01 -1.4336e-01  7.5428e-02 -2.4285e-01\n",
      "  5.0845e-01  6.5961e-02 -6.1576e-02 -1.3920e-01  2.5625e-01  6.5732e-01\n",
      " -3.3939e-01 -2.0979e-01  2.8968e-02  1.2643e-02  5.8778e-02  4.6483e-01\n",
      " -9.7492e-02  5.8118e-02  3.0567e-01  3.3023e-01 -2.8161e-01 -1.4260e-02\n",
      "  4.4502e-01 -5.5786e-02  5.6653e-02  1.1941e-01 -1.8963e-01 -6.9507e-01\n",
      "  1.4026e-01  8.2300e-03 -6.7114e-02 -2.4858e-02  7.1905e-02  1.9854e-01\n",
      " -1.6006e-01  1.5122e-01  8.6351e-02  2.3440e-01 -1.0016e-01 -5.0320e-02\n",
      " -4.6929e-01 -1.6180e-01 -3.1059e-01 -7.8997e-02 -4.7883e-01  2.4786e-01\n",
      " -2.9666e-01  1.1837e-01  2.3957e-01 -4.9512e-01  7.4176e-03  6.7998e-02\n",
      "  5.3889e-01 -4.6161e-02 -2.2584e-01 -2.1110e-01 -2.2687e-01  2.5145e-01\n",
      "  1.6010e-01  2.6152e-01 -3.8447e-01 -9.5562e-02  1.6466e-02  6.0664e-01\n",
      " -3.4478e-01  3.8806e-02  2.2340e-01  9.9682e-02  3.1730e-01 -3.8208e-01\n",
      "  1.4820e-01 -2.8715e-01 -9.4932e-02 -3.0953e-01  9.5717e-02  4.8620e-01\n",
      "  3.6277e-01 -2.7519e-01 -6.7283e-02 -4.8624e-01  1.1835e-01  1.4729e-01\n",
      "  1.1436e-01 -5.1009e-01 -1.0984e-02 -2.0625e-01  8.8184e-02  1.5123e-01\n",
      " -2.7535e-01  1.5052e-01  4.3396e-02  2.3680e-01 -1.7452e-02 -1.8789e-01\n",
      "  5.8767e-01 -2.3496e-01 -6.3975e-02 -1.4580e-01 -3.4655e-01  1.2391e-01\n",
      " -3.9735e-01  8.6459e-03  1.7618e-01  1.4525e-01 -5.8243e-02 -2.4827e-01\n",
      "  4.7140e-01 -4.9669e-02  3.8674e-02  3.0323e-01 -2.4694e-01  1.8770e-01\n",
      " -3.9628e-02  1.5478e-01 -1.6686e-01 -1.6488e-01 -6.5007e-02  9.5371e-01\n",
      "  4.0294e-01  1.9911e-01  1.9991e-01  1.3792e-01  2.4419e-02  4.7980e-01\n",
      "  1.9441e-01  5.7138e-02  2.9218e-02  3.4657e-01  4.0651e-02  5.6484e-01\n",
      "  4.4707e-01 -5.5374e-01 -9.8777e-02 -3.4349e-01  1.1705e-01  8.2749e-01\n",
      "  3.5829e-01  1.2726e-01 -3.5288e-01  6.7834e-03 -2.9921e-01  4.0714e-01\n",
      "  3.1841e-02 -2.8249e-02 -1.1374e-01  3.3910e-01 -2.6651e-01  1.3597e-01\n",
      "[torch.FloatTensor of size 1x50x18]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(xa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll not use any external features for this example, so `x_ext_feats` is a vector of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us step through the `forward` method of the model. Normally we'll just call `trained_model(xq, xa, x_ext_feats)` but to illustrate the steps we'll copy the lines here again and see what happens underneath the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to compute the max-pooled convolutional feature maps for the question. This is a vector of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.3809  0.1988  0.2360  0.1194  0.3365  0.1115  0.1986  0.1210  0.2921  0.2780\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.3465  0.1391  0.2252  0.2028  0.2566  0.2121  0.1145  0.2566 -0.0210  0.0501\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.2870  0.2066  0.1883  0.2801  0.3078  0.1150  0.2241  0.2031  0.3592  0.2815\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.1696  0.5523  0.3096  0.1780  0.1016  0.1626  0.0875  0.1396  0.1777  0.3225\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.2883  0.3741  0.1806  0.3296  0.2664  0.3498  0.1885  0.2927  0.1897  0.1204\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.3060  0.4954  0.1754 -0.0252  0.2306  0.1556  0.2742  0.2156  0.2779  0.2621\n",
       "\n",
       "Columns 60 to 69 \n",
       " 0.2128  0.1529  0.1572  0.3329  0.2616  0.3895  0.5503  0.1169  0.1668  0.1846\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.3015  0.1883  0.1910  0.0919  0.0889  0.2142  0.2740  0.2514  0.2368  0.3541\n",
       "\n",
       "Columns 80 to 89 \n",
       " 0.3160  0.2832  0.2037 -0.0011  0.2704  0.1965  0.2841  0.0728  0.1713  0.2122\n",
       "\n",
       "Columns 90 to 99 \n",
       " 0.0745  0.3086  0.2942  0.2640  0.3694  0.3202  0.2400  0.1647  0.1133  0.1739\n",
       "[torch.FloatTensor of size 1x100]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = trained_model.conv_q.forward(xq)\n",
    "# max pool using q.size()[2] as the window size, which is the length of each convolution feature map\n",
    "q = F.max_pool1d(q, q.size()[2])\n",
    "# reshape max pooled elements into a vector of length equal to the number of feature maps\n",
    "# the max pooling takes one value (the max) out of each convolution feature map\n",
    "q = q.view(-1, trained_model.conv_channels)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we want to compute the max-pooled convolutional feature maps for the answer. This is a vector of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.1825  0.2509  0.1841  0.2155  0.1540  0.0712  0.0202  0.0729  0.2625  0.2019\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.2617  0.1551  0.0740  0.0866  0.1213  0.1076  0.1244  0.0700  0.1704  0.2703\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.3295  0.1632  0.2424  0.3351  0.2256  0.2852  0.2369  0.1707  0.0588  0.2356\n",
       "\n",
       "Columns 30 to 39 \n",
       " 0.2519  0.0645  0.2570  0.1386  0.2201  0.1745  0.2735  0.3343  0.0153 -0.0033\n",
       "\n",
       "Columns 40 to 49 \n",
       " 0.0237  0.1205  0.3471  0.2018  0.2501  0.2470  0.1938  0.3186  0.3065  0.1581\n",
       "\n",
       "Columns 50 to 59 \n",
       " 0.2928  0.1028  0.2428  0.4409  0.0721  0.1606  0.3391  0.1829  0.0680  0.1236\n",
       "\n",
       "Columns 60 to 69 \n",
       " 0.3346  0.1593  0.3588  0.1548  0.1571  0.3544  0.1718  0.2322  0.2835  0.2238\n",
       "\n",
       "Columns 70 to 79 \n",
       " 0.0263  0.1436  0.0468  0.2739  0.2501  0.1943  0.1942  0.1571  0.3216  0.1812\n",
       "\n",
       "Columns 80 to 89 \n",
       " 0.0562  0.1824  0.0222  0.0820  0.2011  0.3240  0.0599  0.2776  0.2596  0.4003\n",
       "\n",
       "Columns 90 to 99 \n",
       " 0.0814  0.1553  0.0909  0.1959 -0.0631  0.1912  0.1817  0.5493  0.3086  0.2260\n",
       "[torch.FloatTensor of size 1x100]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = trained_model.conv_a.forward(xa)\n",
    "a = F.max_pool1d(a, a.size()[2])\n",
    "a = a.view(-1, trained_model.conv_channels)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we join the max-pooled results together with the external features. Note the pre-trained model was trained with external features so we must run the code path with external features, but we can use 0 as the inputs since this is only for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.cat([q, a, x_ext_feats], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run through the rest of the model. Outputting the log-probability of the classes at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0002 -8.6184\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trained_model.combined_feature_vector.forward(x)\n",
    "x = trained_model.combined_features_activation.forward(x)\n",
    "x = trained_model.dropout(x)\n",
    "x = trained_model.hidden(x)\n",
    "x = trained_model.logsoftmax(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9998  0.0002\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the probability of label 0 is 0.9998 while the probability of label 1 is 0.0002."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
